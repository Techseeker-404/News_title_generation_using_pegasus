{"Author": "Rick Merritt\u00a0", "Date": "01.22.2018", "Keywords": "Advanced Technology, Asic, Associations, Boards, Buses, Cloud Computing, Communications And Networking Systems Or Equipment, Computers And Peripherals, Cpld, Design Management, Design Reuse And IP, Design Techniques, Dev Kits, Digital, Events, Fpga, Graphics, Hardware Development, ICs, Industry World, Integrated Development Environments (ides), Manufacturing, Microprocessor, Nanotech, Networking, Operating Systems, Packaging, People, Pld, Programming Languages, Reference Designs, Research & Development, Semiconductor Design & Manufacturing, Semiconductors, Servers, SoC, Software, Standards, Startups", "Article": "  SAN JOSE, Calif. \u00e2\u0080\u0094 Deep neural networks are like a tsunami on the distant horizon. Given their still-evolving algorithms and applications, it\u00e2\u0080\u0099s unclear what changes deep neural nets (DNNs) ultimately will bring. But their successes thus far in translating text and recognizing images and speech make it clear they will reshape computer design, and the changes are coming at a time of equally profound disruptions in how semiconductors are designed and manufactured. The first merchant chips tailored for training DNNs will ship this year. As it can take weeks or months to train a new neural-net model, the chips likely will be some of the largest, and thus most expensive, chunks of commercial silicon made to date. The industry this year may see a microprocessor ship from startup Graphcore that uses no DRAM and one from rival Cerebras Systems that pioneers wafer-level integration. The hefty 2.5-D Nervana chip acquired by Intel is already sampling, and a dozen other processors are in the works. Meanwhile, chip companies from ARM to Western Digital are working on cores to accelerate the inference part of deep neural nets. \u00e2\u0080\u009cI think [2018] will be a coming-out party. We are just starting to see a bunch of ideas being evaluated from lots of companies,\u00e2\u0080\u009d said David Patterson, professor emeritus of the University of California, Berkeley. The trend is so significant that Patterson and co-author John Hennessey devoted a new chapter to it in the latest version of their seminal text on computing, which published last month. The authors provide deep insights into in-house designs such as Google\u2019s TensorFlow Processor (TPU), to which Patterson contributed, as well as Microsoft\u2019s Catapult FPGA and inference blocks in the latest Apple and Google smartphone chips. \u00e2\u0080\u009cThis is a renaissance of computer architecture and packaging. We will see much more interesting computers in the next year than we did in the past decade,\u00e2\u0080\u009d Patterson said. The rise of deep neural nets brought venture capital money back to semiconductors over the last few years. EE Times\u2019 latest Silicon 60 lists seven startups working on some form of neural-networking chips, including two lesser-known names: Cambricon Technologies (Beijing) and Mythic Inc. (Austin, Texas). \u00e2\u0080\u009cWe\u00e2\u0080\u0099re seeing an explosion of new startups with new architectures. I\u00e2\u0080\u0099m tracking 15 to 20 myself \u2026 We haven\u00e2\u0080\u0099t had 15 silicon companies [emerge] in one segment for 10 to 15 years,\u00e2\u0080\u009d said serial entrepreneur Chris Rowen, who left Cadence Design Systems to form Cognite Ventures, a company focused on neural-networking software.  \u00e2\u0080\u009cNvidia will be tough to compete with for training in high-end servers because of its strong software position, and you\u00e2\u0080\u0099d be crazy to go after the cellphone because you have to be good at so many things there, but there may be opportunities at the high and low ends\u00e2\u0080\u009d of the smartphone market, Rowen said. Nvidia did a great job with its latest GPU, Volta, tweaking it to do speed training of DNNs, said Linley Gwennap, principal of market watcher The Linley Group. \u00e2\u0080\u009cBut I certainly don\u00e2\u0080\u0099t think it\u00e2\u0080\u0099s the best possible design,\u00e2\u0080\u009d Gwennap said. Graphcore (Bristol, U.K.) and Cerebras (Los Altos, Calif.) are the top two startups to watch in training chips because they have raised the most money and seem to have the best teams, Gwennap said. Startup Groq, founded by former Google chip designers, claims it will have an inference chip in 2018 that beats rivals by a factor of four in both total operations and inferences per second.     Intel\u00e2\u0080\u0099s Nervana is a large linear algebra accelerator on a silicon interposer next to four 8-Gbyte HBM2 memory stacks.   Click to enlarge. Source: Hennessy and Patterson, \u00e2\u0080\u009cComputer Architecture: A Quantitative Approach\u00e2\u0080\u009d    Intel\u00e2\u0080\u0099s Nervana, called Lake Crest (above), is one of the most-watched custom designs. It executes 16-bit matrix operations with data sharing a single 5-bit exponent provided in the instruction set. As in Nvidia\u00e2\u0080\u0099s Volta, the Lake Crest logic sits on a TSMC CoWoS (chip-on-wafer-on-substrate) interposer next to four HBM2 high-bandwidth memory stacks. The chips are designed to work as a mesh, delivering five to 10 times the performance of Volta. While Microsoft last year drew praise for its use of FPGAs for DNNs, Patterson remains skeptical of that approach. \u00e2\u0080\u009cYou pay a lot for [FPGAs\u00e2\u0080\u0099] flexibility; the programming is really hard,\u00e2\u0080\u009d he said. DSPs also will play a role, Gwennap noted in an analysis late last year . Cadence, Ceva, and Synopsys are all providing DSP cores geared for neural nets, he said.  Page 1: A dozen startups chase deep learning  Page 2: Accelerators lack common benchmarks  Page 3: Still early days for AI algorithms  Page 4: Pioneers expand the application frontier  Page 5: First efforts toward software convergence  Page 6: A look toward the far horizon    The chips are coming at a time when architects have yet to decide how to evaluate them. It\u00e2\u0080\u0099s like the early days of RISC processors, recalled Patterson, when \u00e2\u0080\u009ceach company would say, \u00e2\u0080\u0098You can\u00e2\u0080\u0099t trust the other guy\u00e2\u0080\u0099s benchmark but you can trust mine.\u00e2\u0080\u0099 That didn\u00e2\u0080\u0099t work well.\u00e2\u0080\u009d Back in the day, RISC vendors collaborated on the SPEC benchmarks. Now DNN accelerators need their own defined test suites, covering training and inference as well as standalone and clustered chips across a range of data types. Hearing the call, the Transaction Processing Performance Council (TPC), a group of more than 20 top server and software makers, announced Dec. 12 that it had formed a working group to define hardware and software benchmarks for machine learning. The objective is to create tests that are agnostic to whether accelerators are CPUs or GPUs, said Raghu Nambiar, chairman of the TPC-AI committee. But it\u00e2\u0080\u0099s early days for the group, whose member roster and time frame are still in flux. Baidu, the Google of China, in September 2016 released an open-source benchmark based on its deep-learning workloads for training tasks using 32-bit floating-point math. It updated DeepBench in June to cover inference jobs and the use of 16-bit math. The eight AI workloads defined in the Fathom suite, published by Harvard researchers, support integer and floating-point data. \u00e2\u0080\u009cIt\u00e2\u0080\u0099s a start, but it will take a lot more work to get a thorough benchmark suite people would feel comfortable about,\u00e2\u0080\u009d said Patterson.  \u00e2\u0080\u009cIf we put effort into a good benchmark, all the dollars that go into engineering would be well spent.\u00e2\u0080\u009d Beyond the benchmarks, engineers need to track the still-evolving neural-network algorithms to make sure their designs don\u00e2\u0080\u0099t get left in the dust. \u00e2\u0080\u009cSoftware is always changing, but you need to get hardware out early because it will influence the software \u00e2\u0080\u0094 there\u00e2\u0080\u0099s always this push/pull,\u00e2\u0080\u009d said Karam Chatha, a director of next-generation-core R&D at Qualcomm. So far, the mobile-chip vendor is running neural-net jobs in software on the DSP and GPU cores in its Snapdragon system-on-chip, but some observers expect it will tailor a new block for machine learning as part of a 7-nanometer Snapdragon SoC in 2019.    Qualcomm showed an example of research on a custom DNN accelerator, but today it uses software on generic DSP and GPU cores.   Click to enlarge. Source: Qualcomm    \u00e2\u0080\u009c The marketplace determines which chips work best,\u00e2\u0080\u009d Patterson said. \u00e2\u0080\u009cIt\u00e2\u0080\u0099s brutal, but that\u00e2\u0080\u0099s the excitement of designing computers.\u00e2\u0080\u009d The early players are already getting caught up in this game of chance. For example, Facebook recently demonstrated it could reduce training time from a day to an hour for some jobs by dramatically increasing the number of features packed into so-called batch sizes. That could be bad news for Graphcore, which tries to run all operations in local SRAM, eliminating the latency of external DRAM accesses but also limiting its memory footprint. \u00e2\u0080\u009cThey designed for a small batch size, but the software results from a couple of months ago suggest you want a large batch size. That indicates how quickly things are changing,\u00e2\u0080\u009d Patterson said. On the other hand, Rex Computing believes it is catching a favorable wind. The startup\u00e2\u0080\u0099s SoC, originally designed for high-performance servers, uses a novel scratchpad memory. Rex\u00e2\u0080\u0099s approach eliminates the need to cache data in virtual page tables, a technique GPUs use that adds to their latency, said co-founder Thomas Sohmers. As a result, the Rex chip scales much better than today\u00e2\u0080\u0099s GPUs, especially when handling the popular matrix/vector operations neural nets use, he said. The startup is targeting a June tapeout for a 16-nm SoC with 256 cores that it hopes will deliver 256 Gflops/watt. Meanwhile, researchers are trying everything from 32-bit to single-bit floating-point and integer math to find the most efficient method to compute neural-net results. The one thing they seem to agree on is that it\u00e2\u0080\u0099s best not to have to shift between levels of precision.  Page 1: A dozen startups chase deep learning  Page 2: Accelerators lack common benchmarks  Page 3: Still early days for AI algorithms  Page 4: Pioneers expand the application frontier  Page 5: First efforts toward software convergence  Page 6: A look toward the far horizon    Deep neural networks represent a relatively small branch of the work that has been going on in artificial intelligence for decades. Starting in about 2012, researchers including Yann LeCun at Facebook started showing amazing results using a specific breed of DNNs to recognize images, ultimately with better accuracy than humans. The deep-learning techniques captivated the research community, which is now publishing papers on the field at a feverish rate in search of fresh breakthroughs. DNNs now power commercial services such as Amazon\u00e2\u0080\u0099s Alexa and Google Translate, along with Facebook\u00e2\u0080\u0099s facial recognition. The Web giants and their global rivals are racing to apply the techniques to as many services as possible in a hunt for killer apps. Microsoft hosts two employee-only conferences on AI every year. The last one was attended by 5,000 people, said Marc Tremblay, a former SPARC processor architect now spearheading Microsoft\u00e2\u0080\u0099s work on custom AI chips and systems. Experts acknowledge they don\u00e2\u0080\u0099t fully understand why the current algorithms work so well. Debates rage over the relative effectiveness of an alphabet soup of DNNs such as recurrent (RNN) and convolutional (CNN) neural nets. Meanwhile, new models are still being invented. \u00e2\u0080\u009cThere\u00e2\u0080\u0099s a high probability the algorithms will change in five years. We are gambling that the lowest-level primitives, like matrix multiplies, will be immutable,\u00e2\u0080\u009d said Advanced Micro Devices Fellow Allen Rush, speaking at a recent IEEE symposium on AI. That\u00e2\u0080\u0099s the bet Google made with its TPU, the latest version of which is geared for both training and inference jobs. It is essentially a big array of multiply-accumulate units, running and storing results of linear algebra routines. The Nervana and Graphcore chips are generally expected to follow suit. The current successes with deep neural nets are dominating the larger terrain of AI, said Amir Khosrowshahi, a former brain researcher at Harvard who cofounded Nervana and is now CTO of Intel\u00e2\u0080\u0099s Nervana group. \u00e2\u0080\u009cThings are getting swept under the rug because deep learning has been so successful,\u00e2\u0080\u009d he said at the IEEE symposium. \u00e2\u0080\u009cEveryone is doing CNNs, and this a tragedy \u00e2\u0080\u00a6 don\u00e2\u0080\u0099t assume anything going on now will be there in a year.\u00e2\u0080\u009d Today\u00e2\u0080\u0099s DNNs receive much attention but represent a small subset of the broader field of AI. (Source: Intel)  Although DNNs can recognize images more accurately than humans can, \u00e2\u0080\u009cdata scientists today are forced to spend unacceptable amounts of time preprocessing data, iterating on models and parameters, [and] waiting for training to converge \u00e2\u0080\u00a6 with each step being either too labor- or too compute-intensive,\u00e2\u0080\u009d Khosrowshahi said. Overall, \u00e2\u0080\u009cthe hard problems of AI are still hard,\u00e2\u0080\u009d he added. \u00e2\u0080\u009cThe best researchers can get a robot to unlock a door, but picking up a cup may be harder than winning Alpha Go,\u00e2\u0080\u009d one of the early triumphs of DNNs. In this environment,  web giants such as Facebook and Google are releasing large data sets to get more people working on bleeding-edge issues such as applying recognition to new application areas or data types, such as video.  Page 1: A dozen startups chase deep learning  Page 2: Accelerators lack common benchmarks  Page 3: Still early days for AI algorithms  Page 4: Pioneers expand the application frontier  Page 5: First efforts toward software convergence  Page 6: A look toward the far horizon    As the algorithms evolve, researchers are also pushing the applications frontier of deep learning. Google aims to apply DNNs systematically to problems ranging from auto-captioning pictures for the blind to reading MRI scans and monitoring quality control on the factory floor. \u00c2\u00a0\u00e2\u0080\u009c AI is not a single technology or product,\u00e2\u0080\u009d Jia Li, head of R&D for AI at Google, told the IEEE symposium. \u00e2\u0080\u009cWe need to start with understanding a domain, then gather data, find algorithms, and come up with solutions. We will need a different model for every new problem.\u00e2\u0080\u009d Indeed, work is afoot to apply DNNs to nearly every field, including designing and making chips. Intel compiled a list of more than four dozen possible uses, from online shopping assistants for consumers to automated trading programs for Wall Street. A former IBM researcher now working as a data scientist at Target gave a more sobering perspective on the applications frontier. Most of the retailer\u00e2\u0080\u0099s data is relational, not the unstructured sort best suited for neural nets; only about 10% of Target\u00e2\u0080\u0099s business problems are suitable for DNNs, Shirish Tatikonda said in a brief interview after an event. Nevertheless, the company is exploring the field aggressively, and about 10% of its systems are GPU servers geared for training neural-net models. To scale such a massive effort, Google researchers are exploring what they call AutoML. The idea is to use neural nets to generate models automatically, with no need for data scientists to hand-tune them.    DNN models still vary widely in size, though many recent efforts seek to shrink their memory footprint.   Click to enlarge. Source: Qualcomm    Robotics pioneer Rodney Brooks fears expectations may be getting out of hand. \u00e2\u0080\u009cDeep learning is good, but it\u00e2\u0080\u0099s becoming a hammer people use to hit everything,\u00e2\u0080\u009d he said in a recent talk. Patterson, for his part, remains upbeat. While the broad AI field has failed to live up to some past promises, its current successes in areas such as machine translation are real, he said. \u00e2\u0080\u009cIt\u00e2\u0080\u0099s possible all the low-hanging fruit is picked and there won\u00e2\u0080\u0099t be anything more exciting, but you read about advances almost every week \u00e2\u0080\u00a6 so I think we will find a lot more uses.\u00e2\u0080\u009d  Page 1: A dozen startups chase deep learning  Page 2: Accelerators lack common benchmarks  Page 3: Still early days for AI algorithms  Page 4: Pioneers expand the application frontier  Page 5: First efforts toward software convergence  Page 6: A look toward the far horizon    Amid the early frenzy and fragmentation, even efforts toward software convergence are fragmented. Baidu\u00e2\u0080\u0099s AI research team conducted a survey that found 11 initiatives to bridge the gaps between competing software frameworks for managing neural nets. One of the more promising is the Open Neural Network Exchange (ONNX), an open-source project started by Facebook and Microsoft and recently joined by Amazon. The group released version 1 of the ONNX format in December. It aims to translate neural-net models created with any of a dozen competing software frameworks into a graphical representation. Chip makers can target their hardware at the resulting graphs. That\u00e2\u0080\u0099s good news for startups that can\u00e2\u0080\u0099t afford to write separate software to support competing model frameworks, such as Amazon\u00e2\u0080\u0099s MxNet, Google\u00e2\u0080\u0099s TensorFlow, Facebook\u00e2\u0080\u0099s Caffe2, and Microsoft\u00e2\u0080\u0099s CNTK. A group of more than 30 mainly chip vendors released their preferred option, the Neural Network Exchange Format, on Dec. 20. NNEF aims to offer chip makers an alternative to creating their own, internal formats, as Intel did with Nervana Graph and Nvidia did with TensorRT. Among the alphabet soup of other formats Baidu found are ISAAC, NNVM, Poplar, and XLA. \u00e2\u0080\u009c It may be a bit early to know whether any will emerge as a winning implementation, but we are on a better path, and one of them does need to win eventually,\u00e2\u0080\u009d said Greg Diamos, a senior researcher at\u00c2\u00a0Baidu\u00e2\u0080\u0099s Silicon Valley AI Lab. Among AI frameworks, Amazon claims its MxNet framework and the emerging Gluon API offer the best efficiency. (Source: Amazon)  Separately, Google has started work on software to automate the process of streamlining DNN models so they can run on everything from smartphones to Internet of Things (IoT) nodes. If successful, the effort it could cut 50-Mbyte models down to as little as 500 Kbytes. Google is also exploring ways to do limited model training on handsets by tweaking the top layer of a model or running a process overnight based on data collected during the day. Industry efforts such as SqueezeNet and MobileNet are similarly showing paths to simpler imaging models that are as accurate as their hefty cousins. \u00e2\u0080\u009cWe\u00e2\u0080\u0099ve seen a massive explosion of people using machine learning across a wide range of products,\u00e2\u0080\u009d said Pete Warden, who leads Google\u00e2\u0080\u0099s TensorFlow Lite effort. \u00e2\u0080\u009cPushing down picojoules per operation is what keeps me up at night.\u00e2\u0080\u009d  Page 1: A dozen startups chase deep learning  Page 2: Accelerators lack common benchmarks  Page 3: Still early days for AI algorithms  Page 4: Pioneers expand the application frontier  Page 5: First efforts toward software convergence  Page 6: A look toward the far horizon    When experts squint real hard into the AI future, they see some interesting possibilities. Today we use supervised learning based on manually tuned models. Google\u00e2\u0080\u0099s Warden is among the researchers who see semi-supervised approaches emerging in the near future, with client devices such as handsets handling some learning on their own. The end goal is unsupervised learning \u00e2\u0080\u0094 computers teaching themselves, without help from the engineers who built and programmed them. Along the way, researchers are seeking ways to label data automatically as it is collected by devices such as handsets or IoT nodes. \u00e2\u0080\u009cGoogle says we need a lot of computation now, in this intermediate stage, but once things are automatically labeled you will only need to index new incremental content, [which is] more like how humans process data,\u00e2\u0080\u009d said Janet George, chief data scientist at Western Digital. Unsupervised learning opens a door to an era of accelerated machine intelligence that some see as a digital nirvana. Others worry technology could spin out of control in disastrous ways without human intervention. \u00c2\u00a0\u00e2\u0080\u009cThat\u00e2\u0080\u0099s what scares me,\u00e2\u0080\u009d said Norm Jouppi, the processor veteran who led Google\u00e2\u0080\u0099s TPU project. Meanwhile, academics working on semiconductors have their own long-term visions of future AI chips. With Intel, Graphcore, and Nvidia \u00e2\u0080\u009calready making full-reticle chips, the next step is 3-D,\u00e2\u0080\u009d said Patterson. \u00e2\u0080\u009cWhen Moore\u00e2\u0080\u0099s Law was in full swing, people would chicken out before looking at exotic packaging for fear of reliability and cost problems. Now that Moore\u00e2\u0080\u0099s Law is ending, we will see a lot of experiments in packaging.\u00e2\u0080\u009d The end game here is creating new kinds of transistors that can be stacked on-die in layers of logic and memory. Suman Datta, an electrical engineering professor at Notre Dame, is bullish on negative-capacitance ferroelectric transistors as the basis of such chips. He laid out the landscape in the area at a recent conference dedicated to so-called monolithic 3-D structures. Such designs apply and advance the kinds of gains 3-D NAND flash has made with on-die chip stacks. A team from Berkeley, MIT, and Stanford will present a similarly farsighted architecture at the International Solid-State Circuits Conference in February. The chip (below) stacks resistive RAM (ReRAM) structures on the same die with logic crafted out of carbon nanotubes. Researchers from Berkeley, MIT, and Stanford will report at ISSCC on a novel accelerator using carbon nanotubes, ReRAM, and patterns as computing elements. (Source: UC Berkeley)  Taking inspiration from DNNs, the device is programmed with approximate patterns rather than the deterministic numbers that computers have used to date. This so-called high-dimensional computing uses vectors with tens of thousands of dimensions as compute elements, said Jan Rabaey, a Berkeley professor who contributed to the paper and sits on Intel\u00e2\u0080\u0099s AI advisory board. Such chips could learn from examples and require substantially fewer operations than traditional systems, said Rabaey. A test chip being taped out soon will use arrays of oscillators as analog logic paired with ReRAM cells in an associated memory array. \u00e2\u0080\u009cI dream of engines I can carry that give me guidance on the spot \u00e2\u0080\u00a6 My goal is to push [AI] to operations running at less than 100 millivolts,\u00e2\u0080\u009d Rabaey said at the IEEE AI Symposium. \u00e2\u0080\u009cWe need to rethink how we do computing. We\u00e2\u0080\u0099re moving from algorithm- to data-based systems.\u00e2\u0080\u009d  Page 1: A dozen startups chase deep learning  Page 2: Accelerators lack common benchmarks  Page 3: Still early days for AI algorithms  Page 4: Pioneers expand the application frontier  Page 5: First efforts toward software convergence  Page 6: A look toward the far horizon   \u00e2\u0080\u0094 Rick Merritt, Silicon Valley Bureau Chief, EE Times   "}