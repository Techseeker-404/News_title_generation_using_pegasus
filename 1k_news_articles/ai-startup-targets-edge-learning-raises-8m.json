{"Author": "Nitin Dahad\u00a0", "Date": "01.25.2021", "Keywords": "artificial intelligence, edge AI, edge computing, Fpga, machine learning", "Article": " Artificial intelligence (AI) startup AlphaICs last week announced an $8\u00c2\u00a0 million funding round as it aims to tape out its first edge processor chip next month. With its performance of 8 TOPS at 4W, the company said this is ideal for carrying out edge inference and edge learning. We had reported on their plans back in 2018. Following the latest announcement, EE Times caught up with CEO and executive chairman Pradeep Vajram and co-founder and VP Prashant Trivedi to learn more about the company\u00e2\u0080\u0099s technology and ambitions. Vajram highlighted how the company had re-aligned its ambition to focus on two key capabilities \u00e2\u0080\u0094 edge inference and edge learning. Pradeep Vajram Having now raised a total of $11.5 million, the company said it will use the latest funds to tape-out its first chip, the 8 TOPS RAP-E AI chip, which it is calling Gluon, to develop the software stack and to build system solutions for its target markets. The \u00e2\u0080\u0098RAP\u00e2\u0080\u0099 in AlphaICs\u2019 device nomenclature stands for \u201creal AI processor;\u201d it is based on a proprietary, modular, and scalable architecture to enable AI acceleration for low power edge applications. The chip, which is already fully functional on an FPGA, is scheduled to tape out next month in a TSMC 16nm FinFET process. The Gluon chip does 315 images per second with ResNet-50 on a batch size of one, and 100 images per second with Yolo V2 on a batch size of one. The company said these performances are best in class in terms of images per second per watt and images per second per TOPS, \u00e2\u0080\u009cand much better than the competition, even without any parsing.\u00e2\u0080\u009d Typical power consumption is 4W for these performance figures. Supporting AI frameworks such as TensorFlow, PyTorch and Caffe2, AlphaICs said any trained model can be easily deployed on its Gluon chip. Many startups are now offering what everyone calls \u201cedge AI,\u201d so what is AlphaICs differentiator? \u00c2\u00a0Vajram explained that the company is very much focused on enabling inference and learning on the edge. That means being able to identify and classify both seen and unseen objects at the edge. The current challenges for AI at the edge involve training with large sets of labeled data, the cost of labeling that data and the compute costs, and the significant drop in accuracy with unseen data. Prashant Trivedi with AlphaICs\u2019 FPGA based implementation of its RAP-E (Gluon) chip for edge learning. (Image: AlphaICs) AlphaICs said the performance it is able to achieve with its architecture means it can identify as well as classify objects with less training data. Vajram used the example of a proof of concept built in an FPGA board system that it has developed with a leading defense R&D institution. He said that the details cannot yet be made public until it has submitted the details to the institution, which is likely to be in the next few months. But in essence, the project involves demonstrating that the amount of labeled data needed for training can be reduced and enable edge learning for image classification and detection. In addition, it demonstrates the ability to classify and detect unseen images as and when it encounters them \u00e2\u0080\u0094 this it says is continuous learning on-device, with no need for re-training. With privacy and data availability being a driving factor for edge analytics and AI, this type of learning will be vital for reducing the cost of tasks like people identification, object detection and activity detection at the edge. In addition to facilitating privacy, edge learning also enables automated labeling, and facilitates continuous learning of new scenarios. \u00c2\u00a0Vajram said, \u00e2\u0080\u009cAlphaICs innovative architecture will empower system integrators to create AI solutions, with a short time-to-market; while staying within the systems cost and thermal constraints. This funding will help us bring our first inference co-processor to the market for vision applications with low latency requirements. We are also working with strategic partners to bring innovative solutions to the industrial, automotive, and surveillance markets.\u00e2\u0080\u009d He commented that AlphaICs is attracting interest from a handful of companies looking to evaluate the technology. For example, Visteon showed early interest, and did start evaluating the technology on AlphaICs\u2019 FPGA platform.\u00c2\u00a0 \u201cDue to the delay in our silicon tape out, the discussions with potential customers were put on hold. Now we are ready with tape out, we are looking forward to reinitiating those discussions.\u201d Vajram said early market opportunities for the company are likely to be in surveillance and retail, plus many video analytics applications. \u00e2\u0080\u009cLonger term, cockpit electronics, and driver monitoring systems are also opportunities.\u00e2\u0080\u009d Sateesh Andra, managing director of one of AlphaICs\u00e2\u0080\u0099 lead investors, Endiya Partners, commented, \u00e2\u0080\u009cEdge AI applications in consumer markets like high-end smartphones, wearables as well as enterprise markets like robots, cameras, and sensors will be pervasive in the next few years. AlphaICs RAP accelerates inferencing as well as learning tasks on-device, rather than in a remote data center, delivering benefits like low latency, cost, data privacy, and security. While Nvidia, Google, and startups like Graphcore are poised to dominate data center AI, AlphaICs has the opportunity to be a market leader in enabling AI at the edge.\u00e2\u0080\u009d     Share this:TwitterFacebookLinkedIn "}