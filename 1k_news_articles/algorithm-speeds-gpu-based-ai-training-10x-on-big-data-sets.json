{"Author": "R. Colin Johnson\u00a0", "Date": "12.05.2017", "Keywords": "Cloud Computing, FPGA/PLD/CPLD, Graphics", "Article": " LAKE WALES, Fla. \u00e2\u0080\u0094 IBM Zurich researchers have developed a generic artificial-intelligence preprocessing building block for accelerating Big Data machine learning algorithms by at least 10 times over existing methods. The approach, which IBM presented Monday (Dec. 4) at the Neural Information Processing Systems conference (NIPS 2017) in Long Beach, Calif., uses mathematical duality to cherry-pick the items in a Big Data stream that will make a difference, ignoring the rest. \u00e2\u0080\u009cOur motivation was how to use hardware accelerators, such as GPUs [graphic processing units] and FPGAs [field-programmable gate arrays], when they do not have enough memory to hold all the data points\u00e2\u0080\u009d for Big Data machine learning, IBM Zurich collaborator Celestine D\u00c3\u00bcnner, co-inventor of the algorithm, told EE Times in advance of the announcement. \u00e2\u0080\u009cTo the best of our knowledge, we are first to have generic solution with a 10x speedup,\u00e2\u0080\u009d said co-inventor Thomas Parnell, an IBM Zurich mathematician. \u00e2\u0080\u009cSpecifically, for traditional, linear machine learning models \u00e2\u0080\u0094 which are widely used for data sets that are too big for neural networks to train on \u00e2\u0080\u0094 we have implemented the techniques on the best reference schemes and demonstrated a minimum of a 10x speedup.\u00e2\u0080\u009d IBM Zurich researcher Martin Jaggi at \u00c3\u0089cole Polytechnique F\u00c3\u00a9d\u00c3\u00a9rale de Lausanne (EPFL), also contributed to the machine learning preprocessing algorithm.  For their initial demonstration, the researchers used a single Nvidia Quadro M4000 GPU with 8 gigabytes of memory training on a 30-Gbyte data set of 40,000 photos using a support vector machine (SVM) algorithm that resolves the images into classes for recognition. The SVM algorithm also creates a geometric interpretation of the model learned (unlike neural networks, which cannot justify their conclusions). IBM\u00e2\u0080\u0099s data preprocessing method enabled the algorithm to run in less than a one minute, a tenfold speedup over existing methods using limited-memory training. The key to the technique is preprocessing each data point to see if it is the mathematical dual of a point already processed. If it is, then the algorithm just skips it, a process that becomes increasingly frequent as the data set is processed. \u00e2\u0080\u009cWe calculate the importance of each data point before it is processed by measuring how big the duality gap is,\u00e2\u0080\u009d D\u00c3\u00bcnner said. \u00e2\u0080\u009cIf you can fit your problem in the memory space of the accelerator, then running in-memory will achieve even better results,\u00e2\u0080\u009d Parnell told EE Times. \u00e2\u0080\u009cSo our results apply only to Big Data problems. Not only will it speed up execution time by 10 times or more, but if you are running in the cloud, you won\u00e2\u0080\u0099t have to pay as much.\u00e2\u0080\u009d As Big Data sets grow, such time- and money-saving preprocessing algorithms will become increasingly important, according to IBM. To show that its duality-based algorithm works with arbitrarily large data sets, the company showed an eight-GPU version at NIPS that handles a billion examples of click-through data for web ads. The researchers are developing the algorithm further for deployment in IBM\u00e2\u0080\u0099s Cloud. It will be recommended for Big Data sets involving social media, online marketing, targeted advertising, finding patterns in telecom data, and fraud detection. For details, read Efficient Use of Limited-Memory Accelerators for Linear Learning on Heterogeneous Systems, by D\u00c3\u00bcnner, Parnell, and Jaggi. \u00e2\u0080\u0094 R. Colin Johnson is Advanced Technology Editor at EE Times.  Related articles:  IBM Goes All Out for AI Cray Moves to Lasso \u2018Big Data Deluge\u2019  Beating IoT Big Data With Brain Emulation  IBM Demos In-Memory Massively Parallel Computing IBM Processor Claims New Level of Data Encryption       Share this:TwitterFacebookLinkedIn "}