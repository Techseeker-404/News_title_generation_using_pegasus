{"Author": "Rick Merritt\u00a0", "Date": "05.25.2018", "Keywords": "Academia, Advanced Technology, Analog ICs, Communications And Networking Systems Or Equipment, Computers And Peripherals, Consumer Electronics & Appliances, Design Management, Design Reuse And IP, Design Techniques, Digital, Europe, Events, Geography, Hardware Development, ICs, Industries, Industry World, Internet Of Things, Manufacturing, Memory, Microprocessor, Nanotech, Networking, People, Research & Development, Semiconductor Design & Manufacturing, Semiconductors, SoC, Software, Solid State, Standards, Startups, Storage, Video Processor", "Article": "  ANTWERP, Belgium \u00e2\u0080\u0094 Imec said at its annual event here that it is prototyping a deep-learning inference chip using single-bit precision. The research institute hopes to gather data over the next year on the effectiveness for client devices of the novel data type and architecture\u2013either a processor-in-memory (PIM) or an analog memory fabric. The PIM architecture, explored by academics for decades, is gaining popularity for data-intensive machine-learning algorithms. Startup Mythic and IBM Research are designing two of the most prominent efforts in the field. Many academics are experimenting with 1- to 4-bit data types to trim the heavy memory requirements for deep learning. So far, commercial designs for AI accelerators from Arm and others are focusing on 8-bit and larger data types, in part because programming tools such as Google\u00e2\u0080\u0099s TensorFlow lack support for the smaller data types. Imec had the logic portion of a 40-nm accelerator built in a foundry and is now adding an MRAM layer in its own fab. It simulated the performance of the design using SRAM and estimated design rules for a 5-nm node. The effort is part of an Imec research program still in development with at least two unnamed integrated device manufacturers as partners. The program got its start nearly two years ago and quickly prototyped a PIM design at 65 nm using a form of resistive RAM. The 65-nm chip was not focused on deep-learning algorithms, although it powered a fascinating demo of a computer composing music. It learned patterns using a time-series analysis based on data in the form of music streamed from sensors. The 40-nm Low-Energy Neural Network Accelerator (LENNA) will tackle deep learning head-on, computing and storing binary weights in relatively compact MRAM cells. It will act as a test chip, providing results on the pros and cons of the effectiveness of the architecture, different memories, and binary data types. \u00e2\u0080\u009cOur mission is to define what semiconductor technologies we should develop for machine learning using emerging memories \u00e2\u0080\u0094 we may need process tweaks\u00e2\u0080\u009d to get optimal results, said Diederik Verkest, a distinguished member of technical staff in an interview.   Imec claims that its Lena chips will surpass today\u00e2\u0080\u0099s CPUs and GPUs on inferencing tasks. Click to enlarge. Images: Imec.   \u00e2\u0080\u009cAI will be a driver for how process technology roadmaps evolve, so Imec will put a lot of effort on AI and [PIM architectures] \u00e2\u0080\u0094 how this plays out will be a big deal,\u00e2\u0080\u009d said An Steegen, executive vice president for semiconductor technology and systems at Imec. Indeed, AI marks \u00e2\u0080\u009ca fundamental shift in computing,\u00e2\u0080\u009d said Nigel Toon, chief executive of startup Graphcore, which is rolling out its first chips later this year. \u00e2\u0080\u009cToday\u00e2\u0080\u0099s hardware holds us back; we need something more flexible \u00e2\u0080\u00a6 we want to see [neural-network] models be able to adapt based on experience,\u00e2\u0080\u009d said Toon in a plenary talk at the Imec Tech Forum here. As an example, Toon said that two years ago, Google interns spent $250,000 just on electricity trying to optimize a neural-network model on the search giant\u00e2\u0080\u0099s data centers that use traditional x86 CPUs and Nvidia GPUs. "}