{"Author": "Rick Merritt\u00a0", "Date": "07.20.2018", "Keywords": "Advanced Technology, Cloud Computing, Communications And Networking Systems Or Equipment, Computers And Peripherals, Design Management, Hardware Development, Industry World, Networking, Open Source, Operating Systems, People, Programming Languages, Research & Development, Semiconductors, Servers, Software", "Article": "  MENLO PARK, Calif. \u00e2\u0080\u0094 Web giants are in a kind of Moore\u00e2\u0080\u0099s Law race to build ever-larger distributed computer networks. They are well along in writing new chapters for the computer science history books, but it\u00e2\u0080\u0099s unclear where the trend leads. Take, for example, Facebook. Over the past decade, it has created a global network of 15 large data centers and hundreds of edge-networking sites. The network runs thousands of programs serving billions of users and gets code updates about every two hours. \u00e2\u0080\u009cThe distributed systems you are building are something billions of people will be impacted by daily \u00e2\u0080\u0094 that is both cool and scary,\u00e2\u0080\u009d said Jay Parikh, the head of engineering and infrastructure at Facebook, at the company\u00e2\u0080\u0099s first conference on large distributed software systems at its headquarters here. \u00e2\u0080\u009cEverything we deal with is a distributed systems problem that has never been done before in a scaled environment \u00e2\u0080\u00a6 [including] putting our own cables under the ocean \u00e2\u0080\u0094 things the industry has not had to deal with before,\u00e2\u0080\u009d Parikh told about 200 coders from Facebook and their invited guests. Challenges in the computer and network hardware and the databases and other software that they run \u00e2\u0080\u009cplay off each other \u00e2\u0080\u00a6 [spawning issues in] efficiency, culture, budgeting \u00e2\u0080\u00a6 [because] everything is connected to everything else,\u00e2\u0080\u009d he added. The event included talks from programmers at Amazon Web Services, Google, Lyft, and Shopify, among others. They shared some of their latest techniques for issues ranging from managing globally distributed databases to debugging systems in ways that speed recovery from system outages. For its part, Facebook discussed work on two distributed software systems that it released as open-source \u00e2\u0080\u0094 a framework for quickly pushing configuration changes to millions of servers and code for handling out-of-order memory issues in an operating system\u00e2\u0080\u0099s user space. They are part of a broad cloud-computing software platform that Facebook has created to serve its four widely used applications \u00e2\u0080\u0094 News Feed, Instagram, Messenger, WhatsApp \u00e2\u0080\u0094 and a much smaller group of Oculus software users. An artists conception of a Facebook site housing two 60MW data centers (front) and three 30MW buildings. (Image: Facebook)   Facebook has been slowly replacing off-the-shelf hardware and software components in its network with boxes and programs that it has developed on its own and generally made open-source. For instance, over the last few years, it replaced off-the-shelf database (MySQL), storage (Memcached) programs, and its web-serving language (PHP) with code that it designed in-house and made open-source. The Systems@Scale event here was designed to let Facebook programmers interact with their peers at other companies. The goal is to accelerate the pace of finding common solutions to the large distributed systems problems that they face. In hardware, Facebook now generally has servers and switches built to its own specs using off-the-shelf chips rather than using systems from OEMs. Multiple reports say that it has been hiring semiconductor designers since this spring. A company spokesman offered no comment on the reports but said that it may talk about its chip plans at a September event. Specs for its systems hardware have generally been made open-source through the Open Compute Project that it founded in 2011. Whether it plans to make any future chip designs open-source remains to be seen. The bigger questions are where Facebook and its rivals are taking the future of computing at its heady pace and what walls they might hit. Some U.S. lawmakers believe that they hit a political wall letting outsiders game their automated systems into influencing millions of voters in the last presidential election. Others say that they are gaming consumers, blissfully ignorant that every scrap of data that they share is being sold, so far, with little government oversight. In terms of technologies, the web giants have, for years, been pushing for ever-faster copper and optical networks. They need them both to link ever-larger numbers of servers inside their data centers as well as to like the data centers themselves. For several years, networking engineers have pushed back on their calls to deliver terabit/second networks faster than other commercial users need \u00e2\u0080\u0094 faster, in some cases, than Moore\u00e2\u0080\u0099s Law or even the laws of physics might allow. Facebook\u00e2\u0080\u0099s 15 global data centers are dwarfed by the 18 sites that Amazon had in 2017 (above), each with multiple, separate data centers (yellow circles). (Image: \u00e2\u0080\u009cComputer Architecture: A Quantitative Approach,\u00e2\u0080\u009d Hennessy/Patterson)  Wherever the trend leads, it\u00e2\u0080\u0099s clear that we are all going there quickly. The Facebook conference was housed at a new set of the company\u00e2\u0080\u0099s buildings surrounding its newly constructed headquarters. They sit across the street from the sprawling campus of the former Sun Microsystems that Facebook used to call its main headquarters. A small army of parking attendants and caterers guided and fed attendees amid still other Facebook buildings under construction here. The picture was, as Parikh said, both \u00e2\u0080\u009ccool and scary.\u00e2\u0080\u009d Web giants are building at an unprecedented speed and scale giant automated systems. Billions of consumers and thousands of businesses are gobbling up the services that they run. \u00e2\u0080\u009cThe #1 thing that keeps me up is simply moving fast with a stable infrastructure,\u00e2\u0080\u009d he said. \u00e2\u0080\u0094 Rick Merritt, Silicon Valley Bureau Chief, EE Times    "}