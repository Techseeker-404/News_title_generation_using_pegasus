{"Author": "Gary Hilson\u00a0", "Date": "11.03.2020", "Keywords": "AI, DRAM", "Article": " TORONTO \u00e2\u0080\u0094 Location, location, location is not a mantra limited to real estate. To meet the needs of artificial intelligence (AI) and machine learning applications, it increasingly applies to where data needs to reside, and the memory that stores it. But it\u00e2\u0080\u0099s not just the job of memory vendors to address these placement challenges; other AI stakeholders have a role to play and a big part of the solution is the memory interconnects, even as memory gets moved closer to compute. \u00e2\u0080\u009cWe all work in different aspects of AI,\u00e2\u0080\u009d said Rambus fellow Steve Woo, who recently hosted an online roundtable discussion at the AI Hardware Summit on the challenges and solutions for memory interconnects. Steve Woo, Rambus fellow With events such as Oktoberfest being cancelled due to the COVID-19 pandemic, a beer analogy isn\u00e2\u0080\u0099t a bad one for memory interconnects, said Igor Arsovski, CTO of the ASIC Business Unit at Marvell, who started his career 17 years ago as an SRAM Designer. Attending a beer festival makes a pint easily accessible. \u00e2\u0080\u009cSRAM is like having a beer that\u2019s right next to you. It\u2019s really accessible, it\u2019s low energy to access it and as long as it\u2019s all you need, then you will have a nice high-performance accelerator.\u00e2\u0080\u009d But if it\u00e2\u0080\u0099s not enough memory, you end up going further to get it and it costs a lot more energy to get it, much like having to go further to get beer in larger quantities. Igor Arsovski, CTO of the ASIC business unit at Marvell In memory terms, that might be High Bandwidth Memory (HBM), which is increasingly being adopted for AI, said Arsovski. \u00e2\u0080\u009cIt costs you about 60 times more energy to access that memory. There\u2019s a lot more capacity there, but the bandwidth to access it is also significantly diminished.\u00e2\u0080\u009d The beer analogy can be extended to technologies such as LPDDR, which exceeds SRAM. \u00e2\u0080\u009cThe power is significantly higher, but you can pack even more capacity,\u00e2\u0080\u009d he said. \u00e2\u0080\u009cThat\u2019s like going down the road to your favorite bar where there are kegs of beer.\u00e2\u0080\u009d Where the next generation of accelerators is heading is bringing those kegs right above the accelerator to pack that memory closer through a compute, said Arsovski. His beer analogy outlines the different avenues available in terms of packaging and where you put different pieces of silicon, added David Kanter, executive director of MLCommons, an organization that provides machine learning standards and inference benchmarks, with members encompassing academia and industry. \u00e2\u0080\u009cThat gives us a really nice broad view of the different workloads,\u00e2\u0080\u009d he said. \u00e2\u0080\u009cOne of the things that we\u2019re starting to morph the organization to focus on a bit is building out advisory boards to bring in some of the deep expertise in specific application areas.\u00e2\u0080\u009d David Kanter, executive director, MLCommons When it comes to memory, said Kanter, the overall system context matters. \u00e2\u0080\u009cYou have to think about what you\u2019re trying to do with the system, and that should drive how you think about things.\u00e2\u0080\u009d The die, package, and the board are all elements that must be thought about when it comes to where memory gets placed and connected, he said. \u00e2\u0080\u009cThere are a lot of different corners that you can optimize in terms of array structure, in terms of cell type, in terms of how far away it is.\u00e2\u0080\u009d Understanding where you need bandwidth and non-volatility are also key considerations, said Kanter. \u00e2\u0080\u009cHopefully that will guide you to the right choice.\u00e2\u0080\u009d These considerations are critical for companies that weren\u00e2\u0080\u0099t traditionally part of the overall memory system building process. Google software engineer Sameer Kumar is spending a lot of his time working on compilers and scalable systems where network and memory bandwidth are critical for different machine learning models, including the ability to it at scale in large batch sizes. \u00e2\u0080\u009cAI training has a lot of memory optimization involved,\u00e2\u0080\u009d he said, and it\u00e2\u0080\u0099s the most crucial step in a compiler to get very high efficiency, which means memory needs to be smarter. Sameer Kumar, software engineer, Google Memory interconnects are increasingly important because data movement is really starting to dominate certain phases of AI applications, said Woo. \u00e2\u0080\u009cIt\u00e2\u0080\u0099s a growing problem in terms of the performance and power efficiency.\u00e2\u0080\u009d It\u00e2\u0080\u0099s challenging to increase data rates, as everyone would also like keep doubling the speeds of the data movement and keep doubling the power efficiency as well, he said. \u00e2\u0080\u009cMany of the tricks and the techniques we\u2019ve relied on are no longer available to us or they\u2019re slowing down. There is tremendous opportunity to think about new architectures and to innovate in the way we move data.\u00e2\u0080\u009d Woo said that not only includes innovation in the memory devices themselves, but also in the packages and with new techniques such as stacking, while also keeping mind data security, something Rambus is seeing as growing concern. Rambus has seen a lot of interest in 3D stacking, but without an increase in the bandwidth commensurate with the increased capacity of the stack, there are limits to usability. (Image source: Rambus) Arsovski said Marvell is spending a lot of time with customers building AI systems, providing them information of how much bandwidth they can move per chip edge and how much bandwidth they can have access to memory. \u00e2\u0080\u009cWhat we\u2019ve seen so far is that our customers are needing more memory bandwidth and more I/O of bandwidth,\u00e2\u0080\u009d he said. \u00e2\u0080\u009cIf you look at how the packaging level interconnects at scale, there\u2019s a huge mismatch. We\u2019ve hit this bottleneck right now, and there\u2019s a constant push for high-end chip to chip connections.\u00e2\u0080\u009d From a memory perspective, said Arsovski, the next step for those building AI models that can\u00e2\u0080\u0099t fit on their die is looking at HBM or GDDR as the next step, but there\u00e2\u0080\u0099s also a lot interest in moving to 3D to stack up and try to get more bandwidth because you can only move so much on a chip edge. \u00e2\u0080\u009cCustomers want more and more I/O bandwidth, and we\u2019re hitting a wall with how much we can move on the edge.\u00e2\u0080\u009d Kanter said it\u00e2\u0080\u0099s important to keep in mind that there is \u00e2\u0080\u009chuge diversity\u00e2\u0080\u009d even within the world of machine learning in terms of what people doing, which drives both constraints and variations in the ecosystem. A random lookup into an incredibly large data structure isn\u00e2\u0080\u0099t going to fit into a single node for regular DRAM, which means needing to build incredibly large clusters of systems if you want to actually hold that in memory. \u00e2\u0080\u009cThat has a very different character and property than your classic vision-oriented models,\u00e2\u0080\u009d he said. \u00e2\u0080\u009cIt\u00e2\u0080\u0099s very important is to keep this variety in mind on the memory side.\u00e2\u0080\u009d The interconnect comes in when there\u00e2\u0080\u0099s a need to pull together a lot of memory and compute, said Kanter. \u00e2\u0080\u009cTo train at scale, you really do need an interconnect that is both appropriate for the customer and appropriate for the problem.\u00e2\u0080\u009d The interconnect is going to be particularly important for those on the leading edge, he said. \u00e2\u0080\u009cIf you only want to train with one GPU for a small network, then probably the critical dimension is really the memory bandwidth.\u00e2\u0080\u009d Kumar said more memory bandwidth may enable different kinds of optimization, but if a model is particularly memory bound, it might make sense to bring in more compute. \u00e2\u0080\u009cIf you have more memory throughput available or maybe even more interconnected throughput available, it might make the model design more flexible and enable new features and build the different kinds of models altogether.\u00e2\u0080\u009d Woo said Rambus has seen a lot of interest in 3D stacking, but one the challenges that as you go higher, \u00c2\u00a0it\u2019s harder to keep increasing the bandwidth to go up and down through that stack. \u00e2\u0080\u009cYou end up increasing the capacity of the stack, but if you don\u2019t have that commensurate increase in the bandwidth, then there\u2019s a question about exactly how usable that solution really is.\u00e2\u0080\u009d The Holy Grail, he said, is finding something that has form factor power efficiency benefits of stacking while maintaining the fill frequency so there\u00e2\u0080\u0099s a relatively constant ratio of bandwidth to the capacity and the stack. Both Kumar and Arsovski see a need for a balanced, scalable system with a well-designed software stack. \u00e2\u0080\u009cWe\u2019re describing a human brain-like structure that really scales well,\u00e2\u0080\u009d said Arsovski. It has to be low power with lots of connectivity, and right now, the closest we get to it is through 3D stacking, but there remains power, packaging, and mechanical challenges. \u00e2\u0080\u009cWe need to figure out a very parallel system that\u2019s very low power at each of these layers, so you don\u2019t have to worry about thousands of watts that need to be cooled.\u00e2\u0080\u009d He said it\u00e2\u0080\u0099s time to start looking what the next base building block is for AI systems. \u00e2\u0080\u009cWe\u2019ve been working with transistors and they\u2019ve done a great job up until now. We\u2019re stuck with one technology that we know and love, and we want to keep going with it. It\u2019s almost like we need to rethink the device from the bottom up.\u00e2\u0080\u009d Gary Hilson is a general contributing editor with a focus on memory and flash technologies for EE Times. Related Articles: Emerging Applications Could Transform GDDR Market https://www.eetimes.com/emerging-applications-could-transform-gddr-market/ AI Needs Memory to Get Cozier with Compute https://www.eetimes.com/ai-needs-memory-to-get-cozier-with-compute/ Samsung Doubles HBM Density with Flashbolt https://www.eetimes.com/samsung-doubles-hbm-density-with-flashbolt/ Updated HBM Standard Geared for HPC, Networking https://www.eetimes.com/updated-hbm-standard-geared-for-hpc-networking/     Share this:TwitterFacebookLinkedIn "}