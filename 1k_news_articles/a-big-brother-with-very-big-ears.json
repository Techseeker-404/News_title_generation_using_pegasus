{"Author": "Dylan McGrath\u00a0", "Date": "10.29.2018", "Keywords": "", "Article": "    The prospect of a world of the not-so-distant future where we prance around, barking orders at an omnipresent virtual assistant embedded in everything, is enticing. It's also scary as hell. Like a good many people, Rand Hindi looks at the convergence of the Internet of Things, Deep Learning and audio computing interfaces and imagines a near future of enormous possibilities. With current advances in machine learning and hardware, it won't be long before the capability exists to do voice processing at the edge as opposed to in the cloud, enabling machines to communicate with humans seamlessly and efficiently, using voice and natural language. To Hindi, an entrepreneur and data scientist who is also founder and CEO of voice assistant platform developer Snips, this is an exciting prospect. Once the way we interact with devices becomes as simple as the way we interact with each other, we will no longer need to know anything about how these devices work \u00e2\u0080\u0094 our perception of technology will be that it has completely disappeared. \u201cIt'll be a little bit like electricity \u00e2\u0080\u0094 all around us but invisible to our consciousness,\u201d Hindi said during a keynote address earlier this month at Arm TechCon. Rand Hindi, CEO of SNIPS, speaks at Arm TechCon in San Jose. (Image: Dylan McGrath/EE Times).   Today, the vast majority of voice processing is done in the cloud. When we give an instruction to our Amazon Echo or Google Home device, that instruction gets relayed to the cloud, and the instructions are relayed back to the device to perform the task we have given it. But as Hindi noted in his talk, what's happening now is that voice recognition is moving from the cloud to the edge. (Hindi maintains that automatic voice recognition and simpler models of deel learning can be implemented today on microctrollers. \u201cIt's possible today. It's no longer science fiction,\u201d he said.) It makes sense for a number of reasons to take the relay to and from the cloud out of the equation and perform the computation required locally. First of all, as time goes on, consumers will not tolerate the latency that this extra step requires. But more importantly, there are security implications involved in transferring everything we say to the cloud. \u201cYou don't want the data to be in the cloud because you are giving whoever operates that cloud complete visibility into how your product works,\u201d Hindi said. Partnered Content: Find out how serial Flash tech is evolving to meet the new requirements of Industry 4.0 design.      This brings us to an important point. Because the other thing that is happening, Hindi and others assure us, is that voice recognition technology is moving beyond the hub of an Amazon Echo or Google Cloud and into more consumer products. Eventually, the theory goes, virtually every electronics product that we interact with now will be capable of understanding and executiving upon on our natural language commands. That's all well and good, until you think about the security/privacy implications of a world where everything we say is understood and recorded by the electronics all around us. We are told that the devices we currently surround ourselves with aren't listening until they are spurred to action by certain wake words. But when everything around us is at least potentially listening to us all the time, what are the implications? Stories of espionage through technological means have risen to prominence in recent years. But personally I'm not overly concerned about governments eavesdropping on me (it turns out I'm not all that important in the national or global scale of things). What worries me far more is the idea that the companies that I buy things from nearly everyday are listening in to try and figure out how to sell me more stuff.\u00c2\u00a0\u00c2\u00a0 \u201cAll of the sudden, privacy is a major concern to everyone,\u201d Hindi said. \u201cTwenty years ago, when we first started to buy things online, we didn't care as much about our digital data because we weren't generating that much data. \u201cBut as we started adding more digital stuff, we started creating more data.\u201d In other words: if you have a little bit of data about someone, you don't really know that much about them. But the more you know about them \u00e2\u0080\u0094 their buying habits, friends, organizations, music preferences, etc. \u00e2\u0080\u0094 the more you can influence them. \u201cThe more data you know, the more you know that person and the more you can manipulate them,\u201d Hindi said. \u201cPrivacy is important to prevent mass manipulation of people.\u201d The good news is that people like Hindi and others are thinking about these things. Hindi's company, Snips, provides \u201cprivate by design\u201d \u00c2\u00a0decentralized voice assistant technology, enabling firms and developers to create their own digital assistants which can be pre-trained and then deployed on Raspberry Pi, Android or Linux devices. According to the company, the assistant will then run completely on the device, keeping data safe and private. I can't say I know a great deal about Snips or how it does what it does. But the concept of \u201cprivate by design\u201d seems to me to be a good place to start. Keeping privacy a top priority is essential as the connected and voice activated world of the future is created.      Share this:TwitterFacebookLinkedIn "}