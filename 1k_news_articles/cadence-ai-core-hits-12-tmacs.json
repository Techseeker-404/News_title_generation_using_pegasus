{"Author": "Rick Merritt\u00a0", "Date": "09.19.2018", "Keywords": "Advanced Technology, Communications And Networking Systems Or Equipment, Computers And Peripherals, Design Management, Design Reuse And IP, Digital, Hardware Development, Industry World, Integrated Development Environments (ides), Interface, Logic, Microprocessor, Nanotech, Networking, Research & Development, Semiconductor Design & Manufacturing, Semiconductors, SoC, Software", "Article": "  SAN JOSE, Calif. \u00e2\u0080\u0094 Cadence announced an inference core with up to four times the multiply-accumulate units and up to 12 times the performance of its Vision C5 launched last year. The DNA 100 core supports sparsity in weights and activations and can prune neural networks to deliver higher levels of performance. To date, high-end smartphones have led the way in adopting deep learning for inference jobs with handset SoC vendors, such as Mediatek using Cadence\u2019s Vision P6 core. Designers are now working on AI acceleration in SoCs for surveillance cameras, smart speakers, cars, and AR/VR and IoT devices, said Lazaar Louis, a senior director of product management in Cadence\u00e2\u0080\u0099s Tensilica group. Cadence clocked a 16-nm DNA 100 with 4,000 MACs at up to 2,550 frames/second and up to 3.4 TMACs/W on ResNet-50. A single 16-nm core running at 1 GHz can deliver up to 8 TMACs (12 TMACs using network pruning), and multiple cores can be embedded in an SoC to hit hundreds of TMACs. The numbers would appear to beat Arm\u00e2\u0080\u0099s first ML core that it said in May targets 4.6 tera-operations/second (TOPS) and 3 TOPS/W at 7 nm for high-end handsets.   The DNA 100 can handle weight and activation sparsity and prune neural nets. Click to enlarge. (Source: Cadence)   To deliver the performance, the Cadence core packs an upgraded MAC block and a new custom DSP. It also automates the process of deselecting sparse weights and activations to maximize the use of the MAC array. In addition, it can prune neural nets so that users who opt to retrain them can gain further performance gains. The core supports 8-bit integer as well as 16-bit floating-point and integer formats. It runs graphs created with TensorFlow and Android neural-net frameworks. Cadence is developing support for Facebook\u00e2\u0080\u0099s Glow compiler and associated PyTorch 1.0 framework and has on its roadmap plans to support Amazon\u00e2\u0080\u0099s MxNet and other frameworks. [Partnered Content: Learn how to simplify FPGA reference clocking with Silicon Labs timing solutions] The DNA 100 core will be available in December for select customers with general availability before April. \u00e2\u0080\u0094 Rick Merritt, Silicon Valley Bureau Chief, EE Times    "}