{"Author": "Rick Merritt\u00a0", "Date": "05.02.2018", "Keywords": "Advanced Technology, Asic, Associations, Cloud Computing, Communications And Networking Systems Or Equipment, Computers And Peripherals, Design Management, Design Techniques, Digital, Graphics, Hardware Development, Industry World, Microprocessor, Nanotech, Networking, Research & Development, Semiconductor Design & Manufacturing, Semiconductors, Servers, SoC, Software, Standards, Startups", "Article": "  SAN JOSE, Calif. \u00e2\u0080\u0094 Google and Baidu collaborated with researchers at Harvard and Stanford to define a suite of benchmarks for machine learning. So far, AMD, Intel, two AI startups, and two other universities have expressed support for MLPerf, an initial version of which will be ready for use in August. Today\u00e2\u0080\u0099s hardware falls far short of running neural-networking jobs at the performance levels desired. A flood of new accelerators are coming to market, but the industry lacks ways to measure them. To fill the gap, the first release of MLPerf will focus on training jobs on a range of systems from workstations to large data centers, a big pain point for web giants such as Baidu and Google. Later releases will expand to include inference jobs, eventually extended to include ones run on embedded client systems. \u00e2\u0080\u009cTo train one model we really want to run would take all GPUs we have for two years,\u00e2\u0080\u009d given the size of the model and its data sets, said Greg Diamos, a senior researcher in Baidu\u00e2\u0080\u0099s deep-learning group, giving an example of the issue for web giants. \u00e2\u0080\u009cIf systems become faster, we can unlock the potential of machine learning a lot quicker,\u00e2\u0080\u009d said Peter Mattson, a staff engineer on the Google Brain project who announced MLPerf at a May 2 event. An early version of the suite running on a variety of AI frameworks will be ready to run in about three months. At that time, organizers aim to convene a working group to flesh out a more complete version. \u00e2\u0080\u009cWe\u00e2\u0080\u0099re initially calling it a version 0.5 release \u00e2\u0080\u00a6 we did this with a small team, and now we want the community to put its stamp on a version 1.0 to be something everyone owns,\u00e2\u0080\u009d said Mattson. \u00e2\u0080\u009cWe encourage feedback \u00e2\u0080\u00a6 to suggest workloads, benchmark definitions, and results so we can rapidly iterate\u00e2\u0080\u009d the benchmark. "}