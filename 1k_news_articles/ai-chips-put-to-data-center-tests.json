{"Author": "Rick Merritt\u00a0", "Date": "09.19.2018", "Keywords": "Advanced Technology, Boards, Buses, Cloud Computing, Communications And Networking Systems Or Equipment, Computers And Peripherals, Design Management, Design Reuse And IP, Graphics, Hardware Development, ICs, Industry World, Integrated Development Environments (ides), Manufacturing, Microprocessor, Nanotech, Networking, Packaging, Research & Development, Semiconductor Design & Manufacturing, Semiconductors, Servers, SoC, Software, Startups", "Article": "  MOUNTAIN VIEW, Calif. \u00e2\u0080\u0094 The rubber is about to meet the road in what\u00e2\u0080\u0099s projected to be a $25 billion market for deep-learning accelerators. Data centers are testing multiple chips in the labs now and expect to deploy some next year, probably picking multiple accelerators for different workloads. So far, Graphcore, Habana, ThinCI, and Wave Computing are in the small subset of 50 vendors who have chips that customers are testing in their labs. Representatives from both groups staked out their positions at the AI Hardware Summit here. One issue becoming clear is that \u00e2\u0080\u009cthere\u00e2\u0080\u0099s no such thing as a general-purpose compiler \u00e2\u0080\u0094 these chip architectures are too different,\u00e2\u0080\u009d said Marc Tremblay, a distinguished silicon engineer for Microsoft\u00e2\u0080\u0099s Azure group that operates more than a million servers. The data center giant is developing its own runtime called Lotus to map AI graphs into hardware language. Last week, Facebook announced support for its own approach called Glow, a generic deep-learning compiler. Data centers are hungry for big leaps in AI performance beyond Nvidia\u00e2\u0080\u0099s Volta, the king of training accelerators today. \u00e2\u0080\u009cSome training jobs take 22 days to run on GPUs, and one takes more than two months, but we\u00e2\u0080\u0099d like an answer over lunch,\u00e2\u0080\u009d said Tremblay in a keynote here. One speech-recognition app uses 48 million parameters. Researchers are working on neural nets that generate their own models using non-symmetrical connections that take compute requirements to new levels. \u00e2\u0080\u009cWe need 10 to 50 times more bandwidth to support more esoteric neural nets coming up,\u00e2\u0080\u009d said Tremblay. Today\u00e2\u0080\u0099s GPUs are pricey and power-hungry at about $400,000 for a 16-chip system that requires heat sinks even for its switch chips. Getting linear scaling on clusters of the chips \u00e2\u0080\u009csometimes requires work that our engineers don\u00e2\u0080\u0099t want to do,\u00e2\u0080\u009d he said. For now, Microsoft is using V100 and prior-generation GPUs and \u00e2\u0080\u009cpaying attention\u00e2\u0080\u009d to the T4 chip that Nvidia announced last week. It looks promising for running multiple neural networks simultaneously, noted Tremblay. [Partnered Content: How WinBond\u2019s NOR Flash memory can apply block protection to almost an entire array]  In addition, Microsoft and other data center giants run many deep-learning jobs on their big banks of x86 CPUs. \u00e2\u0080\u009cFor us, it\u00e2\u0080\u0099s often free because the x86 chips are not running all the time,\u00e2\u0080\u009d he said, noting that software optimizations such as a new AI instruction in Intel\u2019s Cascade Lake will drive advances for many years. Looking forward, data centers are likely to adopt multiple accelerators, each mapped to specific workloads that they best fit. Tremblay outlined a variety of speech, vision, language, search, and other AI apps, each with its own latency and throughput requirements. Keynoter Tremblay outlined the landscape of AI silicon. (Image: Microsoft)  Some apps use as many as 20 types of neural nets, making flexibility across models a requirement. They also range from using a single batch for latency-sensitive Bing searches to more than 100 batches for other apps. Thus, Tremblay assigns the chips that he tests a robustness number, a measure of their flexibility. Among the bugaboos, \u00e2\u0080\u009cstartups forget about things like security and virtualization,\u00e2\u0080\u009d he said. \u00e2\u0080\u009cThey don\u00e2\u0080\u0099t need to have everything on Day 1, but eventually we have to get into the class of features that we have with mature CPUs and GPUs.\u00e2\u0080\u009d Overall, the good news in data center AI is that \u00e2\u0080\u009cwe have a long way to go, but progress has been incredible \u00e2\u0080\u00a6 there are lots of innovations coming, and the future is bright for AI,\u00e2\u0080\u009d he concluded. "}