{"Author": "Sally Ward-Foxton\u00a0", "Date": "02.25.2021", "Keywords": "AI, artificial intelligence, data center, Memory", "Article": " Canadian startup Untether AI has an AI inference accelerator card which it claims achieves 2 Peta operations per second (petaOPS) of 8-bit integer performance from four of the company\u00e2\u0080\u0099s RunAI200 ASIC chips. Run in a different mode, the card can also achieve power efficiency of 8 TOPS/W. Working out who is in the lead for performance/power efficiency in the data center AI sector is notoriously difficult, given that offerings span the chip, card and server levels, and that AI benchmarking in general is extremely nuanced. However, the Untether AI card certainly packs a punch. Here\u00e2\u0080\u0099s an overview of Untether\u00e2\u0080\u0099s offering and its secret sauce, the proprietary \u00e2\u0080\u009cat-memory compute\u00e2\u0080\u009d architecture. Large scale inference Founded in Canada in 2018, Untether has operations in Toronto and Waterloo and has raised $27 million in funding to date. The company currently employs 60 people. Untether\u00e2\u0080\u0099s TsunAImi PCIe card features four of the company\u00e2\u0080\u0099s RunAI200 ASIC chips, each offering 502 TOPS for 8-bit integer inference workloads. It also supports 16-bit integer maths, to support some customers\u00e2\u0080\u0099 wish to use INT16 for some layers in order to get the required prediction accuracy, the company said. Run in a slower mode (720 MHz vs 960 MHz for peak performance), the card enters a sweet spot for power consumption, at which point it can achieve 8 TOPS/W. Untether\u2019s TsunAImi PCIe card features four of the company\u2019s ASICs (Image: Untether) \u00e2\u0080\u009cThe silicon and the card are designed for large scale inference workloads,\u00e2\u0080\u009d said Untether CEO Arun Iyengar, in an interview with EE Times. \u00e2\u0080\u009cAs an inference-only solution, we can do native batch size equals one, and we can focus on only integer arithmetic versus having a combination; that gives us a cost and throughput advantage relative to anyone doing mixed [training and inference] chips.\u00e2\u0080\u009d Iyengar said Untether is already getting traction with FinTech companies for enterprise-level acceleration, but the card is equally applicable to hyperscale/cloud environments. \u00e2\u0080\u009cThe basic building blocks that we\u2019ve created allow us to work with pretty much any type of neural net you can throw at us,\u00e2\u0080\u009d he said, citing smart city, smart retail and industrial vision as targets, as well as companies providing AI as a service. At-memory compute Like with many other non-Von Neumann designs for AI accelerators, Untether\u00e2\u0080\u0099s at-memory compute scheme aims to reduce the distance data has to move. Untether\u00e2\u0080\u0099s own study of Von Neumann designs found that for every multiply-accumulate (MAC) operation, more than 90% of the energy was spent moving data from DRAM, getting it to the processor, and getting it through caching to the compute elements. \u00e2\u0080\u009cWe said, well, why don\u2019t we get the compute next to the memory itself?\u00e2\u0080\u009d said Bob Beachler, Untether\u00e2\u0080\u0099s VP products. \u00e2\u0080\u009cAnd so the major innovation that we\u2019ve done is to place the compute elements inside the memory array.\u00e2\u0080\u009d The chip\u00e2\u0080\u0099s memory is standard SRAM, but the processing elements are placed very close by, in a way that maximizes parallelization. This reduces the distance data has to move, which reduces power consumption. Each processing element gets its own dedicated SRAM, leading to a tremendous amount of memory bandwidth; 216 terabytes per second, which Beachler says is two orders of magnitude greater than cutting-edge GPUs. Untether\u00e2\u0080\u0099s at-memory compute scheme intersperses SRAM with compute elements (Image: Untether) Each chip has around 200 MB of SRAM organized into 511 memory banks interspersed between 512 processing elements. Key to the design is the proprietary interconnect: East-West communication (along rows) is via an 8 terabyte/s \u00e2\u0080\u009crotator cuff,\u00e2\u0080\u009d while North-South communication (down columns) is via \u00e2\u0080\u009cdirect row transfer\u00e2\u0080\u009d at 15 TB/s. Neural network math requires multiplying activations with coefficients at large scale. In Untether\u00e2\u0080\u0099s design, compute registers that hold the coefficient data are driven directly by the SRAM, while the rotator cuff is used to move the activation data, allowing processing elements to access activations from its two nearest neighbors on either side, minimizing the distance data has to travel. This rotator cuff interconnect is tailored specifically for AI workloads, Beachler said. Another innovation that saves power is the introduction of zero-detect circuitry. If any activation or coefficient is equal to zero, the hardware will detect this and switch off the MAC unit to avoid wasting power on multiplying anything by zero. Energy is still used to fetch the data, but about half the total energy can still be saved by not doing the MAC operation. This is in addition to any sparsity/pruning schemes that can be run at the tool flow level. Untether\u00e2\u0080\u0099s compute element is a 960-MHz RISC processor with a custom instruction set tailored for AI workloads (Image: Untether) Performance benchmarks According to figures provided by the company, Untether\u00e2\u0080\u0099s TsunAImi card can process 80,000 frames per second for ResNet-50 (four streams of batch=1 data fed to four chips simultaneously \u00e2\u0080\u0093 20,000 fps per chip). In its power-efficient mode, the card can process ResNet-50 at 638 fps per Watt. For BERT-base, TsunAImi\u00e2\u0080\u0099s performance is 12,000 queries per second, or 96 qps/W (8-bit inference, 128 token length). Untether argues that these figures put it ahead of competitors in terms of silicon efficiency (fps/W per mm2 of silicon), even those competitors who use more advanced process nodes than Untether\u00e2\u0080\u0099s 16nm. While working in 16nm helps keep the cost down, there are additional reasons Untether chose this node. \u00e2\u0080\u009cIt\u00e2\u0080\u0099s a proven technology, so we can go to many markets, and it\u00e2\u0080\u0099s also proven from the yield management perspective and the tools perspective,\u00e2\u0080\u009d said Iyengar. \u00e2\u0080\u009cAll of those really enable us to focus on our architectural innovation versus fighting the war on multiple fronts.\u00e2\u0080\u009d Tool flow Untether\u00e2\u0080\u0099s ImAIgine software development kit is designed to shield the user from low-level optimization tasks, with push-button quantization, optimization, physical allocation and multi-chip partitioning. The compiler can optimise physical allocation for power consumption (fit the model into smallest area on the chip) or performance (construct multiple instances to parallelize computation). It also handles multi-chip (and multi-card) partitioning via a heuristic algorithm which analyses the graph to identify areas where there is minimal communication between layers in order to partition the model effectively. Engineering samples of the TsunAImi card are available now, with production quantities coming in Q3. 4-card and 8-card servers will be available from server manufacturer Colfax International. Untether also plans to make its RunAI200 silicon available to customers wanting to build their own cards.     Share this:TwitterFacebookLinkedIn "}