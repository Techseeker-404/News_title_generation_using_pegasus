{"Author": "Michael Wagner, Edge Case Research\u00a0", "Date": "05.07.2019", "Keywords": "", "Article": " At Edge Case Research, we believe that artificial intelligence (AI) has the potential to make everyone safer. For example, in the future, the constant vigilance and superhuman reflexes of AI in self-driving cars could dramatically reduce accident rates. But exceeding the performance of human drivers won\u00e2\u0080\u0099t be easy. It will depend on AI that is safe and reliable across a range of driving conditions. Bias, or systematic error, is a serious challenge to safe AI. Bias can be found in AI-driven products today, such as facial recognition software that performs more accurately for white men than it does for women and people of color. Bias can creep into AI for many reasons. A poor choice of camera could be ill-suited for darker skin. Perhaps training data was captured from the faces of company employees \u00e2\u0080\u0094 who were mostly white and male. What kinds of problems might you find? Here are some examples from analyzing the performance from an open source pedestrian detector. (Source: Edge Case Research) \u00a0   Artificial intelligence is increasingly being used by organizations to make decisions about how people are treated. AIs aren\u2019t human, but in this series on AI Fairness, we examine how they can, and should, be made to behave humanely. The complete list of articles in this series is on page 2. \u00a0  \u00a0 Because AI is used for more safety-critical applications, it\u00e2\u0080\u0099s easy to see how bias could pose risks. Self-driving cars use AI, not unlike facial recognition, for detecting pedestrians. We don\u00e2\u0080\u0099t want cars to be more likely to get into accidents with people that have longer hair, darker skin, or shorter stature. Safety shouldn\u00e2\u0080\u0099t be contingent on how you look. The performance of our pedestrian detector can be described as a trade-off between misses and false alarms. The nature of this trade-off is expressed as a function built by measuring how well the detector handles thousands or millions of human-analyzed images. But this kind of analysis can miss important safety risks. Our detector could perform well enough across the overall population of pedestrians, but it could still miss people in wheelchairs far too frequently. This sort of bias would be clearly unacceptable. Next page:\u00c2\u00a0Bias arises from machine learning itself     Share this:TwitterFacebookLinkedInNext Page "}