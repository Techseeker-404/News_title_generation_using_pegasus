{"Author": "Sally Ward-Foxton\u00a0", "Date": "04.20.2021", "Keywords": "AI, data center, HPC, Startups, Supercomputers", "Article": " Cerebras has crammed even more compute into its wafer-scale chip, in the form of a second-generation Wafer Scale Engine which has migrated to TSMC\u00e2\u0080\u0099s 7nm process node. The Wafer Scale Engine 2 (WSE2), a chip the size of an entire wafer, has 2.6 trillion transistors (by comparison, the largest GPU on the market has around 54 billion transistors). The WSE2 is a staggering 46,225 mm2 of silicon with 40 GB of on-chip memory (an increase from the first generation\u00e2\u0080\u0099s 18 GB), 20 petabytes/s memory bandwidth and 220-petabit/s fabric bandwidth (increased from 9 PB/s and 100 Pb/s, respectively). It is designed for AI workloads in large-scale data center and HPC applications. Describing the WSE2 as the size of a dinner plate, Cerebras CEO Andrew Feldman told EE Times the new chip has more than double the amount of processor cores \u00e2\u0080\u0093 850,000 compared to the previous generation\u00e2\u0080\u0099s 400,000. \u00e2\u0080\u009cWe were able to move about 2.3 X on every performance dimension, so this is a massive step forward,\u00e2\u0080\u009d he said. Cerebras\u00e2\u0080\u0099 7nm wafer-scale chip is the size of a dinner plate (an entire wafer). The chip contains 850,000 processor cores (Source: Cerebras) While the first-generation WSE was built on TSMC\u00e2\u0080\u0099s 16nm process node, the new device has migrated to 7nm. \u00e2\u0080\u009cWhen you invent a new technology that allows you to yield the largest part ever, the question is, have you solved this problem at its root, or did you do it just for the special case at 16nm?\u00e2\u0080\u009d Feldman said. \u00e2\u0080\u009cThe answer is that we solved it for the general case, now we can build wafer scale parts at any geometry.\u00e2\u0080\u009d Moving to the 7nm node poses no additional yield problems, he said. \u00e2\u0080\u009cOur strategy began with the assumption we would face flaws [in the crystalline structure of the silicon],\u00e2\u0080\u009d he said. \u00e2\u0080\u009cWhen there\u2019s a flaw, we don\u2019t throw away the wafer. We map around it just like the data center guys do, when there\u2019s a flaw in one of their servers, they shut it down and they map around it\u00e2\u0080\u00a6 this technique is only possible if you have extreme numbers of identical units, because otherwise the cost of carrying redundant elements is very, very high. And so our yields, even at 7 nm are much, much higher than much smaller parts like the GPU.\u00e2\u0080\u009d Over and above the 850,000 processor cores, there are also 1% redundant cores purely for this eventuality. Connections between nearest neighbors allow defective cores to be skipped with \u00e2\u0080\u009calmost no penalty,\u00e2\u0080\u009d Feldman said. He conceded that patterns of defects that would take out multiple adjacent cores would be harder to overcome, but added that these are \u00e2\u0080\u009cextraordinarily rare. Defects at a beautifully-run fab like TSMC are stochastically distributed, they are randomly distributed, and so our yield is extraordinarily high.\u00e2\u0080\u009d Programming WSE2 As well as moving to a more advanced process node, Cerebras has also improved its micro-architecture following insights gained from customer deployments of the first generation WSE. A great deal of work has also gone into making sure Cerebras\u00e2\u0080\u0099 software is sufficiently robust to move seamlessly between 400,000 and 850,000 cores. The WSE2 is programmed \u00e2\u0080\u009cin exactly the same way you program a GPU,\u00e2\u0080\u009d Feldman said. Unmodified TensorFlow or PyTorch code can be run with just one additional line of code to run on the WSE2. Cerebras\u00e2\u0080\u0099 compiler allocates neural network layers to regions of compute on the chip, then creates a circuit that runs through all the layers and sends data through it. Andrew Feldman (Source: Cerebras) \u00e2\u0080\u009cBy doing it this way, we are able to keep a huge amount of data on the wafer, which is extremely power efficient and extraordinarily low latency,\u00e2\u0080\u009d Feldman said. \u00e2\u0080\u009cThe approach that everybody else uses, building a cluster with lots of little parts, has a great deal of complexity associated with it. You have to figure out how to break up your work into lots of little parts.\u00e2\u0080\u009d Feldman pointed out that clusters of parts require different TensorFlow/PyTorch distributions, careful consideration of memory and bandwidth, and specialized tools. Machine learning rates and hyperparameters have to be adjusted to suit the environment. \u00e2\u0080\u009cYou have to do none of that with our system,\u00e2\u0080\u009d he said. \u00e2\u0080\u009cYou can take your work as designed for one GPU and point it at our system and get this huge performance gain. You can\u2019t take the work for one GPU and point it at 20 GPUs \u00e2\u0080\u0093 you\u00e2\u0080\u0099d have to do all this extra work.\u00e2\u0080\u009d Customer deployments Cerebras already has multiple customers announced in the world of supercomputing, including Argonne National Laboratory, Lawrence Livermore National Laboratory, Pittsburgh Supercomputing Center, and the supercomputing centre at the University of Edinburgh. Argonne has said the first-generation WSE it deployed has reduced experiment turnaround time on cancer prediction models by 300 X, allowing years\u00e2\u0080\u0099 worth of work to be done in months. High-profile enterprise customers include GlaxoSmithKline, and others in the pharmaceutical and financial industries. The company is also seeing interest from hyperscale cloud companies, both domestically and internationally, Feldman said. Availability Cerebras currently has 300 engineers across its offices in Silicon Valley, Toronto, San Diego and Tokyo. The startup has raised more than $475 million with its last round, a Series E in November 2019, valuing it at $2.4 billion (post-money valuation). The WSE2, like its predecessor, will be sold in a rack-mount 26\u00e2\u0080\u009d-tall (15 U) system. The Cerebras CS-2 incorporates the necessary power supplies, fans and liquid cooling to run the WSE2. CS-2 systems will be available in Q3, 2021.     Share this:TwitterFacebookLinkedIn "}