{"Author": "Ann R. Thryft\u00a0", "Date": "05.07.2019", "Keywords": "Business Topics", "Article": "  BOULDER CREEK, Calif. \u00e2\u0080\u0094 Artificial Intelligence or AI has been touted as the Holy Grail of what seems like innumerable applications for automating decision-making. Some of the more typical things AI can do better or faster than people include making movie recommendations on Netflix, detecting cancer, tuning e-commerce and retail websites\u00c2\u00a0for each visitor, and customizing in-car infotainment systems. A few of the more unusual things these automated systems are doing include making better beer, converting thoughts into speech, and creating faster-than-you\u2019d-ever-think-possible death metal. Automated systems have also had some spectacular failures. The self-driving car, proposed as a shining example of what AI can do, failed when a self-driving Uber SUV killed a pedestrian last year. More and more AI systems are being used to make decisions about people and where they can live; what jobs they can have; whether they can be insured and how much their rates will be; what kind of mortgage loan they can get; and whether the Department of Homeland Security thinks, based on their face, that they might be a terrorist. (See our rundown on page 4 of this article on  13 Horror Stories of AI Gone Bad).   \u00a0   Artificial intelligence is increasingly being used by organizations to make decisions about how people are treated. AIs aren\u2019t human, but in this series on AI Fairness, we examine how they can, and should, be made to behave humanely. The complete list of articles in this series is below.  \u00a0  Most of the fairness (or not) of an AI\u2019s decision-making depends on the accuracy and completeness of the test data sets used by the AI\u00e2\u0080\u0099s training algorithm. It also depends on the accuracy of the algorithm itself and how decisions are made about \u201csuccess.\u00e2\u0080\u009d The training algorithm\u2019s optimization strategy can actually amplify bias if it\u2019s optimizing for the maximum overall accuracy of an entire population. Three renowned researchers in AI fairness discuss some of the details in a companion article within this special report, AI Researchers Answer The 5 Big Questions About Fairness. It\u2019s important to understand how the data can be biased, since at first the idea of biased data can sound nonsensical. Sometimes, the data set is biased because it\u2019s incomplete: the data doesn\u2019t reflect the real world. For instance, self-driving car AIs may be trained on sensor data that omits children, people in wheelchairs, or construction workers wearing fluorescent vests, as Michael Wagner describes in a companion story for this report, Bias in AI: Impact on Safety.  Even if the data does reflect the real world, it can still be biased if that \u201creal world\u201d includes past societal inequalities. For instance, some subgroups of the population \u00e2\u0080\u0094 identifiable by any number of categorizations including race, gender, and geographic region have been hired far less often than others for certain jobs due to societal biases \u00e2\u0080\u0094 perhaps they\u2019ve never been hired for that job. That fact will be amplified by the mathematical bias of the algorithm (if, as is usual, it\u2019s optimizing for the general population of the database of who has held that job) and it will it will effectively ignore those minority populations, thus automating the bias. One example of this amplification of data set bias by the AI algorithm occurred a couple of years ago, in experiments\u00c2\u00a0done by researchers at the University of Virginia. They discovered that image data sets of people doing ordinary things like cooking, shopping, and playing sports were highly biased by gender. There were many images of women cooking and shopping but hardly any of them playing sports; the reverse was true for images of men. While this discrepancy may not be surprising, what happened when machine learning algorithms were trained on these data sets was something else. The training algorithms didn\u2019t just reflect those biases \u00e2\u0080\u0094 they amplified them \u00e2\u0080\u0094 to the extent that they often identified pictures of men cooking as pictures of women cooking. The University of Virginia paper contains a photo showing a man, cooking at a stove, clearly labeled as \u201cwoman.\u201d Next page: Algorithm Decisions and Bias in AI   \u00a0  Articles in the AI Fairness Special Projects Series:  Will Machines Ever Learn to Be Fair? SPECIAL PROJECT INTRODUCTION: Artificial intelligence is increasingly being used by organizations to make decisions about how people are treated. AIs aren\u2019t human, but in this series on AI Fairness, we examine how they can, and should, be made to behave humanely. This is our gateway article to our Aspencore Special Project on Fairness in AI. \u00a0  Bias in AI: Impact on Safety Bias doesn\u2019t have to be inherent in the data; it can be the result of a faulty assumption somewhere in the programming process. Resulting errors could be innocuous, but they can also be unwittingly biased \u2013 or even dangerous. \u00a0  Can AI Fairness Be Regulated? The electronics industry is awakening to the issue of AI fairness, even as guidelines are being devised and regulations are being considered. \u00a0  AI Researchers Answer the 5 Big Questions About Fairness Researchers are taking questions about fairness in AI very seriously. Here\u2019s what some of the more prominent among them have to say about it. They\u2019re mostly in agreement on what AI fairness is, and how to define it. Mostly. \u00a0  Reducing Bias in AI Models for Credit and Loan Decisions Some financial institutions have already deployed AIs to make decisions about credit and loans. Early missteps made it clear that bias in AIs is a serious problem \u2013 with serious legal ramifications. \u00a0  Not a Lot of Debiasing, Auditing Tools Yet Recent deployments of AIs have demonstrated that inherent bias is a real problem, one that has to be dealt with. And because it\u2019s a recent problem, there aren\u2019t that many tools to work with \u2013 though there are some, with more to come.     \u00a0     Share this:TwitterFacebookLinkedInNext Page            Ann R. Thryft Ann R. Thryft has written about manufacturing- and electronics-related technologies for Design News, Test & Measurement World, EDN, RTC Magazine, COTS Journal, Nikkei Electronics Asia, Computer Design, and Electronic Buyers' News. She\u00c3\u00a2\u00e2\u0082\u00ac\u00e2\u0084\u00a2s introduced readers to several emerging trends: industrial cybersecurity for operational technology, industrial-strength metals 3D printing, RFID, software-defined radio, early mobile phone architectures, open network server and switch/router architectures, and set-top box system design. At EBN Ann won two independently judged Editorial Excellence awards for Best Technology Feature. Currently, she is the industrial control & automation designline editor at EE Times. She holds a BA in Cultural Anthropology from Stanford University and a Certified Business Communicator certificate from the Business Marketing Association (formerly B/PAA).          1 comments  Post Comment                \t\t\t\t\t\t\t\t\t\tanon9303122\t\t\t\t\t\t\t\t\t\t\u00a0 \t\t\t\t\t\t\t\t\t 2019-05-08 11:27:30    The problem might be that AI is \"too fair\".\u00c2\u00a0 Perhaps a better measure is accuracy and not some nebulous term like \"fairness\".\u00c2\u00a0 This is the crossroads of technology and social engineering.\u00c2\u00a0    Log in to Reply        Leave a Reply Cancel reply \t\t\t\tYou must Register or \t\t\t\tLogin to post a comment.  This site uses Akismet to reduce spam. Learn how your comment data is processed. "}