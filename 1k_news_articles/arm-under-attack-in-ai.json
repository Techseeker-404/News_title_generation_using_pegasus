{"Author": "Rick Merritt\u00a0", "Date": "04.10.2018", "Keywords": "Advanced Technology, Cloud Computing, Communications And Networking Systems Or Equipment, Cpld, Design Management, Design Reuse And IP, Digital, Fpga, Graphics, Hardware Development, ICs, Internet Of Things, Manufacturing, Market Research, Networking, Pld, Semiconductor Design & Manufacturing, Semiconductors, Servers, SoC, Software, Startups", "Article": "  SAN JOSE, Calif. \u00e2\u0080\u0094 Nearly a dozen processor cores for accelerating machine-learning jobs on clients are racing for spots in SoCs, with some already designed into smartphones. They aim to get a time-to-market advantage over processor-IP giant Arm that is expected to announce its own soon. The competition shows that much of the action in machine-learning silicon is shifting to low-power client blocks, according to market watcher Linley Gwennap. However, a race among high-performance chips for the data center is still in its early stages, he told EE Times in a preview of his April 11 keynote for the Linley Processor Conference. \u00e2\u0080\u009cArm has dominated the IP landscape for CPUs and taken over for GPUs as well, but this AI engine creates a whole new market for cores, and other companies are getting a head start,\u00e2\u0080\u009d said Gwennap. The new players getting traction include:  Apple\u00e2\u0080\u0099s Bionic neural engine in the A11 SoC in its iPhone The DeePhi block in Samsung\u00e2\u0080\u0099s Exynos 9810 in the Galaxy S9 The neural engine from China\u00e2\u0080\u0099s Cambricon in Huawei\u00e2\u0080\u0099s Kirin 970 handset The Cadence P5 for vision and AI acceleration in MediaTek\u00e2\u0080\u0099s P30 SoC Possible use of the Movidius accelerator in Intel\u00e2\u0080\u0099s future PC chip sets  The existing design wins have locked up many of the sockets in premium smartphones that represent about a third of the overall handset market. Gwennap expects that AI acceleration will filter down to the rest of the handset market over the next two to three years. Beyond smartphones, cars are an increasingly large market for AI chips. PCs, tablets, and IoT devices will round out the market. To keep pace, Arm announced in February a blanket effort that it calls Project Trillium. But \u00e2\u0080\u009cwhat they need to be competitive is some specific hardware accelerator to optimize power efficiency,\u00e2\u0080\u009d said Gwennap. \u00e2\u0080\u009c\u00e2\u0080\u009cArm is developing that kind of accelerator and plans to release its first product this summer\u2026The fact is that they are behind, which has created an opportunity for the newer companies to jump in.\u00e2\u0080\u009d Last October, Arm announced it had formed a machine-learning group. In February, it provided a few details of its plans. Arm is likely to provide product details at its annual October event in Silicon Valley. But there\u00e2\u0080\u0099s no guarantee that Arm will make up lost ground because there\u00e2\u0080\u0099s not necessarily a close tie between neural net engines and CPUs. Raw performance numbers of client inference accelerators announced so far are just part of the story. (Chart: The Linley Group)    Ultimately, the winning chips in this still-new battle will be the ones with the best combination of performance, power, and die area. \u00e2\u0080\u009cThe problem is that we see the raw performance, but it really comes down to delivered performance on neural networks, so what we need is a good benchmark like the number of images classified per second,\u00e2\u0080\u009d said Gwennap. Baidu was early to release AI benchmarks as open-source, but they have not been widely adopted. The Transaction Processing Council formed a work group late last year to attack the problem, but it has yet to report any progress. \u00e2\u0080\u009cIt\u00e2\u0080\u0099s easy coming up with benchmark, but hard to get companies to agree and compare results \u00e2\u0080\u00a6 and things are changing, so any benchmark will have to evolve to stay relevant,\u00e2\u0080\u009d he said. So far, Gwennap reports that the multi-core v-MP6000 of Videantis has a slight edge in raw performance over its closest rival, Ceva\u00e2\u0080\u0099s NeuPro, which combines a SIMD DSP with systolic MAC array. Other players include Synopsys with its EV64, combining a SIMD DSP with custom logic for activation and pooling. Like Videantis, AImotive\u00e2\u0080\u0099s AIware uses many custom hardware blocks. Among low-cost blocks, VeriSilicon\u00e2\u0080\u0099s VIP8000-O delivers the most raw performance using a GPU with up to eight deep-learning engines. Ironically, Cambricon\u00e2\u0080\u0099s CPU with a small matrix engine offers the lowest performance of announced chips, but it still got a significant design win in the Huawei smartphone. Imagination is also a player with its PowerVR 2NX, a custom, non-GPU architecture with a MAC array. Nvidia hopes to act as a spoiler, making the IP for the NVDLA core in its Xavier processor free and open-source and winning support from Arm. Overall, Gwennap said that as many as 40 companies are now designing customer AI silicon. Many target the data center, where Nvidia\u00e2\u0080\u0099s Volta GPU currently goes largely unchallenged as the training engine of choice by giants including Amazon. \u00e2\u0080\u009cThe competitors we see now are Google\u2019s TPU and Microsoft\u00e2\u0080\u0099s FPGA-based Brainwave that is being deployed widely, but there\u00e2\u0080\u0099s not a lot of merchant alternatives now,\u00e2\u0080\u009d said Gwennap. \u00e2\u0080\u009cWave Computing seems to be ahead of the pack in bringing a new AI data center architecture to production this year.\u00e2\u0080\u009d Wave\u00e2\u0080\u0099s decision to sell full systems suggests that it is targeting second- and third-tier players, not the largest data centers that prefer making their own optimized boxes. Intel\u00e2\u0080\u0099s Nervana recently made clear that it will not have production silicon until 2019. Startup Graphcore suggested that it will announce its chip later this year. Another startup, Cerebrus, remains quiet, while bitcoin ASIC maker BitMain announced plans late last year for an AI chip for data centers. \u00e2\u0080\u009cThere\u00e2\u0080\u0099s a ton of companies working on this kind of stuff,\u00e2\u0080\u009d said Gwennap. \u00e2\u0080\u009cPeople see this as the next gold rush, and they are all trying to jump in.\u00e2\u0080\u009d \u00e2\u0080\u0094 Rick Merritt, Silicon Valley Bureau Chief, EE Times    "}