{"Author": "Rick Merritt\u00a0", "Date": "01.04.2018", "Keywords": "Advanced Technology, Associations, Cloud Computing, Communications And Networking Systems Or Equipment, Computers And Peripherals, Design Management, Digital, Graphics, Hardware Development, Industry World, Memory, Microprocessor, Nanotech, Networking, People, Research & Development, Semiconductor Design & Manufacturing, Semiconductors, Servers, SoC, Software, Solid State, Storage", "Article": "  SAN JOSE, Calif. \u00e2\u0080\u0094 A leading researcher in deep learning praised some of the latest accelerator chips. He also indicated some shortcomings of both the silicon and the software they are supposed to speed up. The results came, in part, from tests using DeepBench, an open-source benchmark for training neural networks using 32-bit floating-point math. Baidu, the Google of China, released DeepBench in September 2016 and updated it in June to cover inference jobs and use of 16-bit math. On some low-level operations such as matrix multiplication, chips with dedicated hardware such as the tensor cores on Nvidia\u2019s Volta GPU can deliver \u00e2\u0080\u009chundreds of TeraFlops \u00e2\u0080\u00a6 several factors faster than the previous generation at 5 to 10 TFlops,\u00e2\u0080\u009d said Greg Diamos, a senior researcher at Baidu\u00e2\u0080\u0099s Silicon Valley AI Lab. However, some low-level operations \u00e2\u0080\u009cused in real apps don\u00e2\u0080\u0099t have enough [data] locality to get full use of these specialized processors, so we either have to live with moderate speedups or change the algorithms.\u00e2\u0080\u009d The Baidu research team is exploring two ways to get more bang for the buck with the new chips. In one effort, researchers are opening controls in their algorithms to take in simultaneous feeds, hoping to boost data parallelism tenfold.   "}