{"Author": "Jeff Bier\u00a0", "Date": "04.29.2021", "Keywords": "AI, artificial intelligence, deep learning, Embedded, neural network, vision systems", "Article": " Presented as a virtual event, the Embedded Vision Summit will examine the latest developments in practical computer vision and edge AI processing. In my role as the summit\u00e2\u0080\u0099s general chair, I reviewed more than 300 great session proposals for the conference. Here are the trends I\u00e2\u0080\u0099m seeing in the embedded vision space. Deep-learning dominance First, surprising no one, deep learning continues to be a dominant force in the field. It\u00e2\u0080\u0099s radically changed what\u00e2\u0080\u0099s possible with computer vision, as well as how we do it. It has made development more data-driven than code-driven, and it\u00e2\u0080\u0099s changed the tools and techniques we use. But data is a pain. Where do you get it? How much of it do you need? How do you get more of it? How do you know you have the right kind of data? Complex vision pipelines Second, despite the deep-learning revolution, product developers are increasingly realizing that deep neural networks (DNNs) do not, by themselves, constitute a product. Real-world products require a complex vision pipeline, often including camera and image processing, DSP, Kalman filters, classical computer vision, and maybe even multiple DNNs, all combined in just the right way to get the results you need.   An explosion of new technologies is bringing AI-enabled vision to more applications than ever before. But is everything moving a bit too fast? We take a look at how the industry is keeping up with the rapid pace of development in our upcoming Embedded Vision Special Project.  Democratized development The third trend is democratization. It\u00e2\u0080\u0099s easier than ever to develop an embedded vision application; thanks to a proliferation of tools and libraries, you don\u00e2\u0080\u0099t have to develop your algorithm from scratch in assembly or C. A great example of this is Edge Impulse, which offers easy to use software tools that allow developers to quickly and easily develop AI models and deploy them on low-cost microprocessors \u00e2\u0080\u0094 with very little coding required. Jeff Bier (Source: Embedded Vision Summit) Also, we\u00e2\u0080\u0099re starting to see suppliers stepping up to support the whole pipeline (Lattice and Qualcomm are good examples here). It\u00e2\u0080\u0099s not hard to imagine a future in which a semiconductor company that has great tools for one component of the pipeline \u00e2\u0080\u0094 DNNs, for example \u00e2\u0080\u0094 but nothing for the other critical pieces will lose market share to competitors that offer more complete solutions. Rise of practical systems Fourth is what I\u00e2\u0080\u0099d call the maturation of the field: We\u00e2\u0080\u0099re moving past the \u00e2\u0080\u009cwow, that\u00e2\u0080\u0099s so cool\u00e2\u0080\u009d stage and are asking how we deploy this technology in ways that are commercially viable and maintainable. Containerization is a great example. The approach has been a best practice in cloud development for over a decade, but we\u00e2\u0080\u0099re starting to see it used to speed development in practical embedded systems, including vision and AI systems (which bring their own challenges, with potentially frequent over-the-air model updates). Similarly, the specters of security and privacy rear their heads. How do we design systems that are secure against hackers and protect user privacy? Relatedly, how do we meet functional safety requirements \u00e2\u0080\u0094 indeed, how do we even test for such things? These are issues that don\u00e2\u0080\u0099t come up in science fair projects but do arise when you\u00e2\u0080\u0099re shipping real products to serious customers. Processors aplenty Fifth is, honestly, an embarrassment of processor riches. A year or two ago, I observed that we were in a Cambrian explosion of processors for AI. Today, if anything, that trend has accelerated and spread: It seems like everybody who makes a processor \u00e2\u0080\u0094 whether it\u00e2\u0080\u0099s a one-dollar MCU or a big, multicore, multi-GHz on-premises server processor \u00e2\u0080\u0094 is targeting edge AI and vision applications. That said, it\u00e2\u0080\u0099s a big space, and processor companies often target different zones in terms of performance, price, and power. For system developers, while it\u00e2\u0080\u0099s great having choice, it can be challenging to choose, especially when you consider not just technical factors (such as performance and power consumption) but other critical issues, such as price, business, and supply chain risk. If there\u00e2\u0080\u0099s a megatrend here, it\u00e2\u0080\u0099s this: We\u00e2\u0080\u0099re living in a golden era of innovation in embedded vision. There\u00e2\u0080\u0099s never been a better time to build vision-based products. To learn more, check out the Embedded Vision Summit for the latest developments in practical computer vision and edge AI.  Jeff Bier is president of consulting firm BDTI, founder of the Edge AI and Vision Alliance, and general chair of the Embedded Vision Summit, which will be held online May 25-28.      Share this:TwitterFacebookLinkedIn "}