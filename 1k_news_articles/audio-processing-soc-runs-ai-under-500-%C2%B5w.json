{"Author": "Sally Ward-Foxton\u00a0", "Date": "01.11.2021", "Keywords": "AI, Audio, DSP, edge AI, IP (intellectual property)", "Article": " DSP Group has debuted new hardware IP for AI acceleration, the nNetLite neural processing unit (NPU), as part of its latest ultra-low-power audio processing SoC, the SmartVoice DBM10. The audio processing SoC is intended for applications with milliwatt power budgets that nonetheless require advanced functionality such as wake-word/voice command detection, sound detection and echo cancellation. The DBM10\u00e2\u0080\u0099s power budget is 400-500 \u00c2\u00b5W when performing neural network inference on typical audio processing models, below the all-important 1mW budget typically specified for always-on features such as wake-word detection in smartphones and IoT devices. \u00e2\u0080\u009cWe are very excited about what the DBM10 brings to current and new customers and partners,\u00e2\u0080\u009d said Ofer Elyakim, CEO of DSP Group, in a statement. \u00e2\u0080\u009cOur team has worked to make the absolute best use of available processing power and memory for low-power AI and ML at the edge \u00e2\u0080\u0094 including developing our own patent-pending weight compression scheme \u00e2\u0080\u0094 while also emphasizing ease of deployment.\u00e2\u0080\u009d New NPU DSP Group\u00e2\u0080\u0099s new nNetLite neural processing unit (NPU) IP was developed in-house as an ultra-low power processor capable of supporting convolutional neural networks (CNNs) as well as other architectures including recurrent neural networks (RNNs) and long short term memory (LSTM) networks. It\u00e2\u0080\u0099s designed for edge-optimized neural networks such as Hello Edge, keyword spotting network designed to be run on small devices such as microcontrollers. Hello Edge running on nNetLite consumes around 1% of the compute power of nNetLite, according to DSP Group. nNetLite\u00e2\u0080\u0099s compiler supports models from all neural network frameworks (TensorFlow Lite or ONNX/NNEF formats). It uses proprietary model size optimization and compression techniques, including quantization (1-16 bit), post-training model pruning and patent-pending lossless entropy compression algorithms. The result is that neural network models as big as tens of megabytes can be ported to the small device without significant accuracy loss. Dual-core design The audio processing SoC is a dual-core design featuring a DSP alongside the neural network processor, which allows algorithms to be partitioned between both cores as appropriate. Most voice algorithms today include a software module that performs feature extraction \u00e2\u0080\u0093 that is, it looks for features in the data that are used to simplify processing further down the line by reducing the number of variables involved while still describing the data accurately. Feature extraction is typically performed by the DSP, with results fed into the neural network running on the NPU. The DBM10\u00e2\u0080\u0099s dual-core design features a DSP and a neural processing unit (NPU) (Image: DSP Group) The nNetLite can run neural networks on information for several input sensors simultaneously \u00e2\u0080\u0093 perhaps from a microphone, an accelerometer and a bone-conducting microphone. The sensor data can be aggregated by the DSP before being passed to the NPU. As part of DSP Group\u00e2\u0080\u0099s SmartVoice line, the DBM10 already has an established ecosystem of third-party algorithm providers. According to DSP Group, some of these vendors have already started running their algorithms and neural networks on the nNetLite, while Tier 1 customers have already begun designing with it. The DBM10 is available now. \u00a0 Related articles:  DSP Group Acquires SoundChip, Enters Earbuds Market  \u00a0     Share this:TwitterFacebookLinkedIn "}