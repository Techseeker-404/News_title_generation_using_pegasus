{"Author": "Alexei Andreev and Jeff Peters, Autotech Ventures\u00a0", "Date": "08.21.2018", "Keywords": "Academia, Computers And Peripherals, ICs, Research & Development, Semiconductor Design & Manufacturing, Semiconductors", "Article": "    How confident are we that algorithms of tomorrow are a good fit for existing semiconductor chips or new computational fabrics under development? With algorithmic advances outpacing hardware advances, even the most advanced deep-learning model could be deployed on a chip as small as a $5 Raspberry Pi. Which solves faster: a top modern algorithm on a 1980s processor or a 1980s algorithm running on a top modern processor? The surprising answer is that often it\u00e2\u0080\u0099s a new algorithm on an old processor. While Moore\u00e2\u0080\u0099s Law gets a lot of attention as the driver of rapid advance of electronics, it is only one of the drivers. We regularly forget that algorithmic advances beat Moore\u00e2\u0080\u0099s Law in many cases. Professor Martin Groetschel observed that a linear programming problem that would take 82 years to solve in 1988 could be solved in one minute in 2003. Hardware accounted for 1,000 times speedup, while algorithmic advance accounted for 43,000 times. Similarly, MIT professor Dimitris Bertsimas showed that the algorithm speedup between 1991 and 2013 for mixed integer solvers was 580,000 times, while the hardware speedup of peak supercomputers increased only a meager 320,000 times. \u00c2\u00a0Similar results are rumored to take place in other classes of constrained optimization problems and prime number factorization. What does that mean for AI? In the past half-decade there has been an explosion of artificial intelligence (AI) in the academic, industrial and startup world. Probably the greatest inflection point occurred when AlexNet, a team from University of Toronto, destroyed the competition in the 2012 the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) using deep-learning. Deep-learning has since become a key formulation for AI implementations. Advances in computer vision sprawled into natural language processing and other AI domains. Smart speakers, real-time computer translation, robotic hedge funds, and web reference engines do not surprise us anymore. [Partnered Content: Can complex AI software be executed on mobile devices?] AI has also become a driving force in the transportation industry (our investment focus at Autotech Ventures). We\u00e2\u0080\u0099ve observed great promise in advanced driving assistance systems (ADAS), autonomous driving, fleet inspection, manufacturing quality control, in-vehicle human-machine interface, and more. So far, we have made several investments into startups developing various AI solutions in this domain including ADAS and autonomous driving, visual inspection, and edge computing. As we analyze these opportunities, the interplay between algorithm and hardware is a key consideration in our decision-making. Public attention on AI hardware  The inflection point for deep-learning based AI was followed by ravenous demand for graphical processing units (GPUs). Because of their parallel computation prowess, GPUs happen to be surprisingly efficient for the logic employed by deep-learning algorithms. Nvidia, the major manufacturer of GPUs, \u00e2\u0080\u009cdecimated\u00e2\u0080\u009d their competition, and saw their stock price climb 20-fold from 2013 to 2018. Of course, Nvidia competitors are trying to catch up. Qualcomm, Arm, and others have focused attention on AI chip design, while Intel acquired startup AI chip company Nervana Systems. Google, Facebook, Apple, and Amazon have all waded in to build their own AI processing units for their data centers as well as other initiatives. There are also a handful of startups who jumped in when they saw the opportunity (e.g. Graphcore, Mythic, Wave Computing, Cerebras, SambaNova) to build a better-designed Turing machine. A few others like D-wave Systems and IBM are actively exploring post-Turing architectures. For most chip development efforts, the goal is to catch up to or beat Nvidia. However, most of these processors \u00e2\u0080\u0094 to the best of our knowledge \u00e2\u0080\u0094 are being designed for the AI algorithms of today. We are going through an ongoing Cambrian explosion of alternative AI chip designs despite their enormous upfront development costs. The end game for AI is so breathtakingly large that industry participants are willing to invest heavily, holistically matching hardware to the underlying mathematical algorithms. But, how confident are we that algorithms of tomorrow are a good fit for existing semiconductor chips or new computational fabrics under development? Given the rate and the magnitude of algorithm evolution, many of those alternative AI chip designs may become obsolete even before their commercial releases. We speculate that AI algorithms of tomorrow might demand different compute architectures, memory resources, data-transfer capabilities, etc.     Even though deep-learning frameworks have been known for a long time, it has only been recently that they have been put into practice thanks to those Moore\u00e2\u0080\u0099s Law-predicted hardware advancements. The original mathematics were not necessarily designed for engineering practice as early researchers could not dream of computational power that $1,000 can buy today. Today, many implementations, driving toward accuracy, simply add more layers (e.g. making them \u00e2\u0080\u009cdeeper\u00e2\u0080\u009d) or add more data using the original mathematics. This simply strains the computational capacity of the GPU. Only a small fraction of researchers is focusing on the hard problem of improving the underlying mathematical and algorithmic frameworks. There is opportunity to recognize and capitalize on these novel mathematical advances. We have observed methods that trim redundant mathematical operations to reduce computation time, squeeze the convolutions to smaller matrices to reduce memory requirements, or binarize weighting matrices to simplify mathematical operations. These are the first forays into the algorithmic advances that are starting to outpace hardware advances. For example, DeepScale* , spun-out of UC Berkeley research, \u00e2\u0080\u009csqueezes\u00e2\u0080\u009d AI for advanced driver assistance systems (ADAS) and autonomous driving onto automotive grade-chips (as opposed to GPUs). Their neural network models have demonstrated 30 times speedup compared to leading object detection models using algorithms alone while reducing energy and memory footprint enough to run on existing hardware in just a couple of years. Another example of such algorithmic leapfrogging came from researchers from the Allen Institute of Artificial Intelligence. Using a novel mathematical approach employing binarization of neural networks, they showed that they can drastically increase speed as well as reduce power and memory requirements. This enables even the most advanced deep-learning model to be deployed on a chip as small as a $5 Raspberry Pi. The researchers recently spun out the algorithms and processing tools as XNOR.ai*  to deploy AI on edge devices and drive further algorithmic advances for AI. The interesting point is that the new binarization frameworks fundamentally change the type of optimal processing logic. Instead of the 32-bit floating point convolutions required to solve neural networks they need only bit counting operations \u00e2\u0080\u0093 shifting the balance of power away from GPUs. Further, the computational resource requirements can be further reduced if these algorithms are matched by specifically-designed chip.\u00c2\u00a0\u00c2\u00a0 And algorithmic advances will not stop. Sometimes it takes years or even decades to invent (or perhaps discover) new algorithms. Those breakthroughs cannot be predicted in the same way as computational advances driven by Moore\u00e2\u0080\u0099s Law. They are non-deterministic by nature. But when they happen, the entire landscape shifts often turning incumbent dominant players into vulnerable prey. \u00c2\u00a0 Black Swans  In his bestseller The Black Swan: The Impact of the Highly Improbable, Nassim Nicolas Taleb showed that optimal decision-making depends heavily on whether the analyzed process is unpredictable or uncertain. In other words, are we dealing with \u00e2\u0080\u009cknown unknowns\u00e2\u0080\u009d or \u00e2\u0080\u009cunknown unknowns\u00e2\u0080\u009d? \u00c2\u00a0Algorithmic innovations are fundamentally unknown unknowns. Betting on those developments requires constant attention due to their non-deterministic time-to-discovery and unpredictable impact. However, the field of applied mathematics in general, and domain of artificial intelligence in particular, saw several disruptive algorithmic discoveries in the last two decades that \u00e2\u0080\u0094 alongside with GPUs \u00e2\u0080\u0094 \u00c2\u00a0brought AI from an obscure field of research to the forefront of commercialization.  We recognize and appreciate potential for computational \u00e2\u0080\u009cBlack Swans\u00e2\u0080\u009d that would make existing chip architectures obsolete or reshuffle their relative popularity overnight. For us, these Black Swans could finally unleash safe autonomous vehicles as well as many other, as of yet unknown, use cases. \u00e2\u0080\u0094 Alexei Andreev PhD is a managing director and Jeff Peters PhD is a principal at Autotech Ventures, a venture capital firm focusing on transportation-related technology. (Disclosure: DeepScale and XNOR.ai are Autotech Ventures portfolio companies.)       Share this:TwitterFacebookLinkedIn "}